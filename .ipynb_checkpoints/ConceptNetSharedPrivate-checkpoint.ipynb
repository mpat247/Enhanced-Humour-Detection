{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8821c8cc-cf5f-4246-a6bb-0bd04e6a6f5b",
   "metadata": {},
   "source": [
    "# Enhanced Shared-Private BERT with ConceptNet for Humor Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f4d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset...\n",
      "[INFO] Dataset loaded successfully. Total samples: 100000\n",
      "\n",
      "[INFO] Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       100000 non-null  object\n",
      " 1   label      100000 non-null  int64 \n",
      " 2   type       100000 non-null  object\n",
      " 3   concepts   100000 non-null  object\n",
      " 4   relations  100000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 3.8+ MB\n",
      "None\n",
      "\n",
      "[INFO] Sample Rows:\n",
      "                                                text  label             type  \\\n",
      "0              Good he answer ok allow trial worker.      0  body punchlines   \n",
      "1  I met the world's tallest and the world's heav...      1  body punchlines   \n",
      "2  I was at a barber shop in Bangkok and I asked ...      1  body punchlines   \n",
      "3                        Its build card time factor.      0  body punchlines   \n",
      "4  A cashier asks a French guy would you like a b...      1  body punchlines   \n",
      "\n",
      "                                            concepts relations  \n",
      "0                             ['trial worker', 'he']        []  \n",
      "1                             ['i', 'the other day']        []  \n",
      "2  ['rainbow swirls', 'me,\"well', 'bangkok', 'the...        []  \n",
      "3                     ['its build card time factor']        []  \n",
      "4  ['french', 'the french guy', 'this year', 'a c...        []  \n",
      "\n",
      "[INFO] Checking for missing values:\n",
      "text         0\n",
      "label        0\n",
      "type         0\n",
      "concepts     0\n",
      "relations    0\n",
      "dtype: int64\n",
      "\n",
      "[INFO] Distribution of humor types:\n",
      "body punchlines    25000\n",
      "news headlines     25000\n",
      "puns               25000\n",
      "storylines         25000\n",
      "Name: type, dtype: int64\n",
      "\n",
      "[INFO] Distribution of labels (humorous vs. non-humorous):\n",
      "1    50164\n",
      "0    49836\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the dataset path\n",
    "DATASET_PATH = './data/shared_private_dataset.csv'\n",
    "DATASET_PATH = './concept_dataset.csv'\n",
    "\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(\"[INFO] Loading dataset...\")\n",
    "    dataset = pd.read_csv(DATASET_PATH)\n",
    "    print(f\"[INFO] Dataset loaded successfully. Total samples: {len(dataset)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"[ERROR] Dataset file not found at {DATASET_PATH}\")\n",
    "\n",
    "# Step 2: Display basic information about the dataset\n",
    "print(\"\\n[INFO] Dataset Info:\")\n",
    "print(dataset.info())\n",
    "\n",
    "# Step 3: Display sample rows\n",
    "print(\"\\n[INFO] Sample Rows:\")\n",
    "print(dataset.head())\n",
    "\n",
    "# Step 4: Check for missing values\n",
    "print(\"\\n[INFO] Checking for missing values:\")\n",
    "missing_values = dataset.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Step 5: Check the distribution of humor types\n",
    "if 'type' in dataset.columns:\n",
    "    print(\"\\n[INFO] Distribution of humor types:\")\n",
    "    humor_type_distribution = dataset['type'].value_counts()\n",
    "    print(humor_type_distribution)\n",
    "else:\n",
    "    print(\"[WARNING] Column 'type' not found in the dataset.\")\n",
    "\n",
    "# Step 6: Check label distribution\n",
    "if 'label' in dataset.columns:\n",
    "    print(\"\\n[INFO] Distribution of labels (humorous vs. non-humorous):\")\n",
    "    label_distribution = dataset['label'].value_counts()\n",
    "    print(label_distribution)\n",
    "else:\n",
    "    print(\"[WARNING] Column 'label' not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057f42fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>concepts</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good he answer ok allow trial worker.</td>\n",
       "      <td>0</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['trial worker', 'he']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I met the world's tallest and the world's heav...</td>\n",
       "      <td>1</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['i', 'the other day']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was at a barber shop in Bangkok and I asked ...</td>\n",
       "      <td>1</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['rainbow swirls', 'me,\"well', 'bangkok', 'the...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Its build card time factor.</td>\n",
       "      <td>0</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['its build card time factor']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A cashier asks a French guy would you like a b...</td>\n",
       "      <td>1</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['french', 'the french guy', 'this year', 'a c...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label             type  \\\n",
       "0              Good he answer ok allow trial worker.      0  body punchlines   \n",
       "1  I met the world's tallest and the world's heav...      1  body punchlines   \n",
       "2  I was at a barber shop in Bangkok and I asked ...      1  body punchlines   \n",
       "3                        Its build card time factor.      0  body punchlines   \n",
       "4  A cashier asks a French guy would you like a b...      1  body punchlines   \n",
       "\n",
       "                                            concepts relations  \n",
       "0                             ['trial worker', 'he']        []  \n",
       "1                             ['i', 'the other day']        []  \n",
       "2  ['rainbow swirls', 'me,\"well', 'bangkok', 'the...        []  \n",
       "3                     ['its build card time factor']        []  \n",
       "4  ['french', 'the french guy', 'this year', 'a c...        []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# # Load spaCy model for concept extraction\n",
    "# print(\"[INFO] Loading spaCy model...\")\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# print(\"[INFO] spaCy model loaded successfully.\")\n",
    "\n",
    "# # Function to extract concepts from text\n",
    "# def extract_concepts(text, index=None, total=None, print_every=10000):\n",
    "#     \"\"\"\n",
    "#     Extracts key concepts from the input text using NER and noun phrase extraction.\n",
    "\n",
    "#     Parameters:\n",
    "#         text (str): The input text.\n",
    "#         index (int, optional): Current index of the text in the dataset.\n",
    "#         total (int, optional): Total number of entries in the dataset.\n",
    "#         print_every (int): Print status every `print_every` rows.\n",
    "\n",
    "#     Returns:\n",
    "#         List[str]: A list of unique concepts extracted from the text.\n",
    "#     \"\"\"\n",
    "#     # Print progress updates every `print_every` rows\n",
    "#     if index is not None and total is not None and index % print_every == 0:\n",
    "#         print(f\"[INFO] Processing row {index}/{total}...\")\n",
    "\n",
    "#     doc = nlp(text)\n",
    "#     concepts = set()\n",
    "\n",
    "#     # Extract named entities\n",
    "#     for ent in doc.ents:\n",
    "#         concepts.add(ent.text.lower())\n",
    "\n",
    "#     # Extract noun phrases\n",
    "#     for chunk in doc.noun_chunks:\n",
    "#         concepts.add(chunk.text.lower())\n",
    "\n",
    "#     return list(concepts)\n",
    "\n",
    "# # Apply the function with explicit indexing for progress updates\n",
    "# print(\"[INFO] Extracting concepts for the entire dataset...\")\n",
    "\n",
    "# concepts = []  # Initialize a list to store concepts for each row\n",
    "# for idx, row in dataset.iterrows():\n",
    "#     # Extract concepts and append them to the list\n",
    "#     concepts.append(extract_concepts(row['text'], index=idx, total=len(dataset), print_every=10000))\n",
    "\n",
    "# # Add the extracted concepts as a new column in the dataset\n",
    "# dataset['concepts'] = concepts\n",
    "\n",
    "# print(\"[INFO] Concepts extraction completed.\")\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821ed7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset saved successfully as 'concept_dataset.csv' in the root folder.\n",
      "[INFO] Preview of the dataset:\n",
      "                                                text  label             type  \\\n",
      "0              Good he answer ok allow trial worker.      0  body punchlines   \n",
      "1  I met the world's tallest and the world's heav...      1  body punchlines   \n",
      "2  I was at a barber shop in Bangkok and I asked ...      1  body punchlines   \n",
      "3                        Its build card time factor.      0  body punchlines   \n",
      "4  A cashier asks a French guy would you like a b...      1  body punchlines   \n",
      "\n",
      "                                            concepts relations  \n",
      "0                             ['trial worker', 'he']        []  \n",
      "1                             ['i', 'the other day']        []  \n",
      "2  ['rainbow swirls', 'me,\"well', 'bangkok', 'the...        []  \n",
      "3                     ['its build card time factor']        []  \n",
      "4  ['french', 'the french guy', 'this year', 'a c...        []  \n",
      "[INFO] Condensed dataset created with equal distribution across types.\n",
      "[INFO] Condensed dataset saved successfully as 'concept_dataset_sampled.csv'.\n",
      "[INFO] Condensed dataset shape: (100000, 5)\n",
      "[INFO] Preview of the condensed dataset:\n",
      "                                                text  label             type  \\\n",
      "0  Whats red and white and sits in a tree? A sani...      1  body punchlines   \n",
      "1  Discover idea traditional like another reach t...      0  body punchlines   \n",
      "2                           Seat center spend place.      0  body punchlines   \n",
      "3  Place rather about garden reality local can in...      0  body punchlines   \n",
      "4     I ve been cleaning my shoes with Vodka removed      1  body punchlines   \n",
      "\n",
      "                                         concepts relations  \n",
      "0            ['what', 'a sanitary owl', 'a tree']        []  \n",
      "1  ['another', 'discover idea', 'task treatment']        []  \n",
      "2                        ['seat center', 'place']        []  \n",
      "3                              ['garden reality']        []  \n",
      "4                      ['i', 'vodka', 'my shoes']        []  \n"
     ]
    }
   ],
   "source": [
    "# # Save the updated dataset to a CSV file in the root folder\n",
    "# output_filename = \"concept_dataset.csv\"\n",
    "# dataset.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "# print(f\"[INFO] Dataset saved successfully as '{output_filename}' in the root folder.\")\n",
    "\n",
    "# # Display the first few rows of the dataset\n",
    "# print(\"[INFO] Preview of the dataset:\")\n",
    "# print(dataset.head())\n",
    "\n",
    "# # Condense the dataset to around 100,000 rows with equal distribution across the 'type' column\n",
    "# desired_rows_per_type = 25000  # Approximate number of rows per type\n",
    "# grouped = dataset.groupby('type')  # Group the dataset by the 'type' column\n",
    "\n",
    "# # Sample rows equally from each type\n",
    "# condensed_dataset = grouped.apply(\n",
    "#     lambda x: x.sample(n=min(len(x), desired_rows_per_type), random_state=42)\n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "# print(\"[INFO] Condensed dataset created with equal distribution across types.\")\n",
    "\n",
    "# # Save the condensed dataset to a CSV file\n",
    "# condensed_output_filename = \"concept_dataset_sampled.csv\"\n",
    "# condensed_dataset.to_csv(condensed_output_filename, index=False, encoding='utf-8')\n",
    "# print(f\"[INFO] Condensed dataset saved successfully as '{condensed_output_filename}'.\")\n",
    "\n",
    "# # Update the dataset variable to the condensed dataset\n",
    "# dataset = condensed_dataset\n",
    "\n",
    "# # Print the shape of the condensed dataset\n",
    "# print(f\"[INFO] Condensed dataset shape: {dataset.shape}\")\n",
    "\n",
    "# # Display the first few rows of the condensed dataset\n",
    "# print(\"[INFO] Preview of the condensed dataset:\")\n",
    "# print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bbf6d-f714-44a3-abbb-1e3da1ac90f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded existing cache from 'concept_cache.json'.\n",
      "[INFO] Extracting unique concepts from the dataset...\n",
      "[INFO] Extracted 46 unique concepts.\n",
      "[INFO] Fetching ConceptNet relationships for unique concepts...\n",
      "[INFO] Processing batch 1 with 46 concepts...\n",
      "[INFO] Cache updated and saved to 'concept_cache.json'.\n",
      "[INFO] Mapping relationships back to dataset rows...\n",
      "[INFO] Relationship mapping completed.\n",
      "[INFO] Dataset saved successfully as 'relation_dataset.csv'.\n",
      "[INFO] Displaying the first few rows of the updated dataset:\n",
      "                                                text  label             type  \\\n",
      "0              Good he answer ok allow trial worker.      0  body punchlines   \n",
      "1  I met the world's tallest and the world's heav...      1  body punchlines   \n",
      "2  I was at a barber shop in Bangkok and I asked ...      1  body punchlines   \n",
      "3                        Its build card time factor.      0  body punchlines   \n",
      "4  A cashier asks a French guy would you like a b...      1  body punchlines   \n",
      "\n",
      "                                            concepts  \\\n",
      "0                             ['trial worker', 'he']   \n",
      "1                             ['i', 'the other day']   \n",
      "2  ['rainbow swirls', 'me,\"well', 'bangkok', 'the...   \n",
      "3                     ['its build card time factor']   \n",
      "4  ['french', 'the french guy', 'this year', 'a c...   \n",
      "\n",
      "                                           relations  \n",
      "0  [{'@id': '/a/[/r/RelatedTo/,/c/en/t’s/n/,/c/en...  \n",
      "1  [{'@id': '/a/[/r/RelatedTo/,/c/en/i/,/c/en/pro...  \n",
      "2  [{'@id': '/a/[/r/IsA/,/c/en/r/n/wn/communicati...  \n",
      "3  [{'@id': '/a/[/r/RelatedTo/,/c/en/i/,/c/en/pro...  \n",
      "4  [{'@id': '/a/[/r/Synonym/,/c/en/fo/n/wikt/en_1...  \n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops for Jupyter compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Asynchronous function to query ConceptNet API for a given concept\n",
    "async def get_conceptnet_relations(session, concept, limit=3):\n",
    "    \"\"\"\n",
    "    Queries ConceptNet API to retrieve relationships for a given concept and filters fields.\n",
    "\n",
    "    Parameters:\n",
    "        session (aiohttp.ClientSession): The aiohttp session for making requests.\n",
    "        concept (str): The concept to query (e.g., \"money\").\n",
    "        limit (int): Number of top relations to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of filtered dictionaries representing the relationships.\n",
    "    \"\"\"\n",
    "    concept_query = f\"/c/en/{concept.replace(' ', '_')}\"  # Format concept for ConceptNet query\n",
    "    url = f\"https://api.conceptnet.io/query?node={concept_query}&other=/c/en&limit={limit}\"\n",
    "\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            response.raise_for_status()  # Raise an error for HTTP errors\n",
    "            data = await response.json()\n",
    "            edges = data.get(\"edges\", [])\n",
    "            \n",
    "            # Filter and simplify edges\n",
    "            simplified_edges = [\n",
    "                {\n",
    "                    \"start\": edge.get(\"start\", {}).get(\"label\", \"unknown\"),\n",
    "                    \"end\": edge.get(\"end\", {}).get(\"label\", \"unknown\"),\n",
    "                    \"relation\": edge.get(\"@id\", \"\").split(\"/\")[-2],  # Extract relation type\n",
    "                    \"weight\": edge.get(\"weight\", 0),\n",
    "                }\n",
    "                for edge in edges\n",
    "            ]\n",
    "            return simplified_edges\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to fetch data for concept '{concept}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to fetch relationships for all unique concepts asynchronously\n",
    "async def fetch_relations_async(concepts, cache, limit=3):\n",
    "    \"\"\"\n",
    "    Fetch relationships for all unique concepts asynchronously using aiohttp.\n",
    "\n",
    "    Parameters:\n",
    "        concepts (List[str]): List of unique concepts to query.\n",
    "        cache (dict): Cache to store and reuse previously fetched results.\n",
    "        limit (int): Number of relationships to fetch per concept.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated cache with fetched and simplified relationships.\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for concept in concepts:\n",
    "            if concept not in cache:  # Only fetch if not in cache\n",
    "                tasks.append(get_conceptnet_relations(session, concept, limit))\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        for concept, relations in zip(concepts, results):\n",
    "            cache[concept] = relations\n",
    "\n",
    "    return cache\n",
    "\n",
    "# Batch processing utility\n",
    "def batch(iterable, n=1):\n",
    "    \"\"\"\n",
    "    Splits an iterable into batches of size n.\n",
    "\n",
    "    Parameters:\n",
    "        iterable (iterable): The iterable to split.\n",
    "        n (int): The batch size.\n",
    "\n",
    "    Returns:\n",
    "        generator: Generator yielding batches of size n.\n",
    "    \"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    while True:\n",
    "        batch_items = list(islice(iterable, n))\n",
    "        if not batch_items:\n",
    "            break\n",
    "        yield batch_items\n",
    "\n",
    "# Load cache from file if it exists\n",
    "try:\n",
    "    with open('concept_cache.json', 'r') as cache_file:\n",
    "        concept_cache = json.load(cache_file)\n",
    "    print(\"[INFO] Loaded existing cache from 'concept_cache.json'.\")\n",
    "except FileNotFoundError:\n",
    "    concept_cache = defaultdict(list)\n",
    "    print(\"[INFO] No existing cache found. Starting fresh.\")\n",
    "\n",
    "# Step 1: Deduplicate all concepts in the dataset\n",
    "print(\"[INFO] Extracting unique concepts from the dataset...\")\n",
    "unique_concepts = set([concept for row in dataset['concepts'] for concept in row])\n",
    "print(f\"[INFO] Extracted {len(unique_concepts)} unique concepts.\")\n",
    "\n",
    "# Step 2: Fetch relationships using asynchronous batching\n",
    "print(\"[INFO] Fetching ConceptNet relationships for unique concepts...\")\n",
    "batch_size = 250  # Number of concepts to process in a batch\n",
    "for batch_idx, concept_batch in enumerate(batch(unique_concepts, batch_size), start=1):\n",
    "    print(f\"[INFO] Processing batch {batch_idx} with {len(concept_batch)} concepts...\")\n",
    "    concept_cache = asyncio.run(fetch_relations_async(concept_batch, cache=concept_cache, limit=3))\n",
    "\n",
    "# Save the updated cache to a file\n",
    "with open('concept_cache.json', 'w') as cache_file:\n",
    "    json.dump(concept_cache, cache_file)\n",
    "print(\"[INFO] Cache updated and saved to 'concept_cache.json'.\")\n",
    "\n",
    "# Step 3: Map cached relationships back to rows in the dataset\n",
    "print(\"[INFO] Mapping relationships back to dataset rows...\")\n",
    "dataset['relations'] = dataset['concepts'].apply(\n",
    "    lambda concepts: [relation for concept in concepts for relation in concept_cache.get(concept, [])]\n",
    ")\n",
    "print(\"[INFO] Relationship mapping completed.\")\n",
    "\n",
    "# Step 4: Save the updated dataset\n",
    "output_filename = \"relation_dataset.csv\"\n",
    "dataset.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "print(f\"[INFO] Dataset saved successfully as '{output_filename}'.\")\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"[INFO] Displaying the first few rows of the updated dataset:\")\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85eef766-9bc5-447f-a0ad-612f1dc139f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing dataset in chunks of 100 rows...\n",
      "[INFO] Processing chunk 1...\n",
      "[INFO] Processing chunk 2...\n",
      "[INFO] Processing chunk 3...\n",
      "[INFO] Processing chunk 4...\n",
      "[INFO] Processing chunk 5...\n",
      "[INFO] Processing chunk 6...\n",
      "[INFO] Processing chunk 7...\n",
      "[INFO] Processing chunk 8...\n",
      "[INFO] Processing chunk 9...\n",
      "[INFO] Processing chunk 10...\n",
      "[INFO] Processing chunk 11...\n",
      "[INFO] Processing chunk 12...\n",
      "[INFO] Processing chunk 13...\n",
      "[INFO] Processing chunk 14...\n",
      "[INFO] Processing chunk 15...\n",
      "[INFO] Processing chunk 16...\n",
      "[INFO] Processing chunk 17...\n",
      "[INFO] Processing chunk 18...\n",
      "[INFO] Processing chunk 19...\n",
      "[INFO] Processing chunk 20...\n",
      "[INFO] Processing chunk 21...\n",
      "[INFO] Processing chunk 22...\n",
      "[INFO] Processing chunk 23...\n",
      "[INFO] Processing chunk 24...\n",
      "[INFO] Processing chunk 25...\n",
      "[INFO] Processing chunk 26...\n",
      "[INFO] Processing chunk 27...\n",
      "[INFO] Processing chunk 28...\n",
      "[INFO] Processing chunk 29...\n",
      "[INFO] Processing chunk 30...\n",
      "[INFO] Processing chunk 31...\n",
      "[INFO] Processing chunk 32...\n",
      "[INFO] Processing chunk 33...\n",
      "[INFO] Processing chunk 34...\n",
      "[INFO] Processing chunk 35...\n",
      "[INFO] Processing chunk 36...\n",
      "[INFO] Processing chunk 37...\n",
      "[INFO] Processing chunk 38...\n",
      "[INFO] Processing chunk 39...\n",
      "[INFO] Processing chunk 40...\n",
      "[INFO] Processing chunk 41...\n",
      "[INFO] Processing chunk 42...\n",
      "[INFO] Processing chunk 43...\n",
      "[INFO] Processing chunk 44...\n",
      "[INFO] Processing chunk 45...\n",
      "[INFO] Processing chunk 46...\n",
      "[INFO] Processing chunk 47...\n",
      "[INFO] Processing chunk 48...\n",
      "[INFO] Processing chunk 49...\n",
      "[INFO] Processing chunk 50...\n",
      "[INFO] Processing chunk 51...\n",
      "[INFO] Processing chunk 52...\n",
      "[INFO] Processing chunk 53...\n",
      "[INFO] Processing chunk 54...\n",
      "[INFO] Processing chunk 55...\n",
      "[INFO] Processing chunk 56...\n",
      "[INFO] Processing chunk 57...\n",
      "[INFO] Processing chunk 58...\n",
      "[INFO] Processing chunk 59...\n",
      "[INFO] Processing chunk 60...\n",
      "[INFO] Processing chunk 61...\n",
      "[INFO] Processing chunk 62...\n",
      "[INFO] Processing chunk 63...\n",
      "[INFO] Processing chunk 64...\n",
      "[INFO] Processing chunk 65...\n",
      "[INFO] Processing chunk 66...\n",
      "[INFO] Processing chunk 67...\n",
      "[INFO] Processing chunk 68...\n",
      "[INFO] Processing chunk 69...\n",
      "[INFO] Processing chunk 70...\n",
      "[INFO] Processing chunk 71...\n",
      "[INFO] Processing chunk 72...\n",
      "[INFO] Processing chunk 73...\n",
      "[INFO] Processing chunk 74...\n",
      "[INFO] Processing chunk 75...\n",
      "[INFO] Processing chunk 76...\n",
      "[INFO] Processing chunk 77...\n",
      "[INFO] Processing chunk 78...\n",
      "[INFO] Processing chunk 79...\n",
      "[INFO] Processing chunk 80...\n",
      "[INFO] Processing chunk 81...\n",
      "[INFO] Processing chunk 82...\n",
      "[INFO] Processing chunk 83...\n",
      "[INFO] Processing chunk 84...\n",
      "[INFO] Processing chunk 85...\n",
      "[INFO] Processing chunk 86...\n",
      "[INFO] Processing chunk 87...\n",
      "[INFO] Processing chunk 88...\n",
      "[INFO] Processing chunk 89...\n",
      "[INFO] Processing chunk 90...\n",
      "[INFO] Processing chunk 91...\n",
      "[INFO] Processing chunk 92...\n",
      "[INFO] Processing chunk 93...\n",
      "[INFO] Processing chunk 94...\n",
      "[INFO] Processing chunk 95...\n",
      "[INFO] Processing chunk 96...\n",
      "[INFO] Processing chunk 97...\n",
      "[INFO] Processing chunk 98...\n",
      "[INFO] Processing chunk 99...\n",
      "[INFO] Processing chunk 100...\n",
      "[INFO] Processing chunk 101...\n",
      "[INFO] Processing chunk 102...\n",
      "[INFO] Processing chunk 103...\n",
      "[INFO] Processing chunk 104...\n",
      "[INFO] Processing chunk 105...\n",
      "[INFO] Processing chunk 106...\n",
      "[INFO] Processing chunk 107...\n",
      "[INFO] Processing chunk 108...\n",
      "[INFO] Processing chunk 109...\n",
      "[INFO] Processing chunk 110...\n",
      "[INFO] Processing chunk 111...\n",
      "[INFO] Processing chunk 112...\n",
      "[INFO] Processing chunk 113...\n",
      "[INFO] Processing chunk 114...\n",
      "[INFO] Processing chunk 115...\n",
      "[INFO] Processing chunk 116...\n",
      "[INFO] Processing chunk 117...\n",
      "[INFO] Processing chunk 118...\n",
      "[INFO] Processing chunk 119...\n",
      "[INFO] Processing chunk 120...\n",
      "[INFO] Processing chunk 121...\n",
      "[INFO] Processing chunk 122...\n",
      "[INFO] Processing chunk 123...\n",
      "[INFO] Processing chunk 124...\n",
      "[INFO] Processing chunk 125...\n",
      "[INFO] Processing chunk 126...\n",
      "[INFO] Processing chunk 127...\n",
      "[INFO] Processing chunk 128...\n",
      "[INFO] Processing chunk 129...\n",
      "[INFO] Processing chunk 130...\n",
      "[INFO] Processing chunk 131...\n",
      "[INFO] Processing chunk 132...\n",
      "[INFO] Processing chunk 133...\n",
      "[INFO] Processing chunk 134...\n",
      "[INFO] Processing chunk 135...\n",
      "[INFO] Processing chunk 136...\n",
      "[INFO] Processing chunk 137...\n",
      "[INFO] Processing chunk 138...\n",
      "[INFO] Processing chunk 139...\n",
      "[INFO] Processing chunk 140...\n",
      "[INFO] Processing chunk 141...\n",
      "[INFO] Processing chunk 142...\n",
      "[INFO] Processing chunk 143...\n",
      "[INFO] Processing chunk 144...\n",
      "[INFO] Processing chunk 145...\n",
      "[INFO] Processing chunk 146...\n",
      "[INFO] Processing chunk 147...\n",
      "[INFO] Processing chunk 148...\n",
      "[INFO] Processing chunk 149...\n",
      "[INFO] Processing chunk 150...\n",
      "[INFO] Processing chunk 151...\n",
      "[INFO] Processing chunk 152...\n",
      "[INFO] Processing chunk 153...\n",
      "[INFO] Processing chunk 154...\n",
      "[INFO] Processing chunk 155...\n",
      "[INFO] Processing chunk 156...\n",
      "[INFO] Processing chunk 157...\n",
      "[INFO] Processing chunk 158...\n",
      "[INFO] Processing chunk 159...\n",
      "[INFO] Processing chunk 160...\n",
      "[INFO] Processing chunk 161...\n",
      "[INFO] Processing chunk 162...\n",
      "[INFO] Processing chunk 163...\n",
      "[INFO] Processing chunk 164...\n",
      "[INFO] Processing chunk 165...\n",
      "[INFO] Processing chunk 166...\n",
      "[INFO] Processing chunk 167...\n",
      "[INFO] Processing chunk 168...\n",
      "[INFO] Processing chunk 169...\n",
      "[INFO] Processing chunk 170...\n",
      "[INFO] Processing chunk 171...\n",
      "[INFO] Processing chunk 172...\n",
      "[INFO] Processing chunk 173...\n",
      "[INFO] Processing chunk 174...\n",
      "[INFO] Processing chunk 175...\n",
      "[INFO] Processing chunk 176...\n",
      "[INFO] Processing chunk 177...\n",
      "[INFO] Processing chunk 178...\n",
      "[INFO] Processing chunk 179...\n",
      "[INFO] Processing chunk 180...\n",
      "[INFO] Processing chunk 181...\n",
      "[INFO] Processing chunk 182...\n",
      "[INFO] Processing chunk 183...\n",
      "[INFO] Processing chunk 184...\n",
      "[INFO] Processing chunk 185...\n",
      "[INFO] Processing chunk 186...\n",
      "[INFO] Processing chunk 187...\n",
      "[INFO] Processing chunk 188...\n",
      "[INFO] Processing chunk 189...\n",
      "[INFO] Processing chunk 190...\n",
      "[INFO] Processing chunk 191...\n",
      "[INFO] Processing chunk 192...\n",
      "[INFO] Processing chunk 193...\n",
      "[INFO] Processing chunk 194...\n",
      "[INFO] Processing chunk 195...\n",
      "[INFO] Processing chunk 196...\n",
      "[INFO] Processing chunk 197...\n",
      "[INFO] Processing chunk 198...\n",
      "[INFO] Processing chunk 199...\n",
      "[INFO] Processing chunk 200...\n",
      "[INFO] Processing chunk 201...\n",
      "[INFO] Processing chunk 202...\n",
      "[INFO] Processing chunk 203...\n",
      "[INFO] Processing chunk 204...\n",
      "[INFO] Processing chunk 205...\n",
      "[INFO] Processing chunk 206...\n",
      "[INFO] Processing chunk 207...\n",
      "[INFO] Processing chunk 208...\n",
      "[INFO] Processing chunk 209...\n",
      "[INFO] Processing chunk 210...\n",
      "[INFO] Processing chunk 211...\n",
      "[INFO] Processing chunk 212...\n",
      "[INFO] Processing chunk 213...\n",
      "[INFO] Processing chunk 214...\n",
      "[INFO] Processing chunk 215...\n",
      "[INFO] Processing chunk 216...\n",
      "[INFO] Processing chunk 217...\n",
      "[INFO] Processing chunk 218...\n",
      "[INFO] Processing chunk 219...\n",
      "[INFO] Processing chunk 220...\n",
      "[INFO] Processing chunk 221...\n",
      "[INFO] Processing chunk 222...\n",
      "[INFO] Processing chunk 223...\n",
      "[INFO] Processing chunk 224...\n",
      "[INFO] Processing chunk 225...\n",
      "[INFO] Processing chunk 226...\n",
      "[INFO] Processing chunk 227...\n",
      "[INFO] Processing chunk 228...\n",
      "[INFO] Processing chunk 229...\n",
      "[INFO] Processing chunk 230...\n",
      "[INFO] Processing chunk 231...\n",
      "[INFO] Processing chunk 232...\n",
      "[INFO] Processing chunk 233...\n",
      "[INFO] Processing chunk 234...\n",
      "[INFO] Processing chunk 235...\n",
      "[INFO] Processing chunk 236...\n",
      "[INFO] Processing chunk 237...\n",
      "[INFO] Processing chunk 238...\n",
      "[INFO] Processing chunk 239...\n",
      "[INFO] Processing chunk 240...\n",
      "[INFO] Processing chunk 241...\n",
      "[INFO] Processing chunk 242...\n",
      "[INFO] Processing chunk 243...\n",
      "[INFO] Processing chunk 244...\n",
      "[INFO] Processing chunk 245...\n",
      "[INFO] Processing chunk 246...\n",
      "[INFO] Processing chunk 247...\n",
      "[INFO] Processing chunk 248...\n",
      "[INFO] Processing chunk 249...\n",
      "[INFO] Processing chunk 250...\n",
      "[INFO] Processing chunk 251...\n",
      "[INFO] Processing chunk 252...\n",
      "[INFO] Processing chunk 253...\n",
      "[INFO] Processing chunk 254...\n",
      "[INFO] Processing chunk 255...\n",
      "[INFO] Processing chunk 256...\n",
      "[INFO] Processing chunk 257...\n",
      "[INFO] Processing chunk 258...\n",
      "[INFO] Processing chunk 259...\n",
      "[INFO] Processing chunk 260...\n",
      "[INFO] Processing chunk 261...\n",
      "[INFO] Processing chunk 262...\n",
      "[INFO] Processing chunk 263...\n",
      "[INFO] Processing chunk 264...\n",
      "[INFO] Processing chunk 265...\n",
      "[INFO] Processing chunk 266...\n",
      "[INFO] Processing chunk 267...\n",
      "[INFO] Processing chunk 268...\n",
      "[INFO] Processing chunk 269...\n",
      "[INFO] Processing chunk 270...\n",
      "[INFO] Processing chunk 271...\n",
      "[INFO] Processing chunk 272...\n",
      "[INFO] Processing chunk 273...\n",
      "[INFO] Processing chunk 274...\n",
      "[INFO] Processing chunk 275...\n",
      "[INFO] Processing chunk 276...\n",
      "[INFO] Processing chunk 277...\n",
      "[INFO] Processing chunk 278...\n",
      "[INFO] Processing chunk 279...\n",
      "[INFO] Processing chunk 280...\n",
      "[INFO] Processing chunk 281...\n",
      "[INFO] Processing chunk 282...\n",
      "[INFO] Processing chunk 283...\n",
      "[INFO] Processing chunk 284...\n",
      "[INFO] Processing chunk 285...\n",
      "[INFO] Processing chunk 286...\n",
      "[INFO] Processing chunk 287...\n",
      "[INFO] Processing chunk 288...\n",
      "[INFO] Processing chunk 289...\n",
      "[INFO] Processing chunk 290...\n",
      "[INFO] Processing chunk 291...\n",
      "[INFO] Processing chunk 292...\n",
      "[INFO] Processing chunk 293...\n",
      "[INFO] Processing chunk 294...\n",
      "[INFO] Processing chunk 295...\n",
      "[INFO] Processing chunk 296...\n",
      "[INFO] Processing chunk 297...\n",
      "[INFO] Processing chunk 298...\n",
      "[INFO] Processing chunk 299...\n",
      "[INFO] Processing chunk 300...\n",
      "[INFO] Processing chunk 301...\n",
      "[INFO] Processing chunk 302...\n",
      "[INFO] Processing chunk 303...\n",
      "[INFO] Processing chunk 304...\n",
      "[INFO] Processing chunk 305...\n",
      "[INFO] Processing chunk 306...\n",
      "[INFO] Processing chunk 307...\n",
      "[INFO] Processing chunk 308...\n",
      "[INFO] Processing chunk 309...\n",
      "[INFO] Processing chunk 310...\n",
      "[INFO] Processing chunk 311...\n",
      "[INFO] Processing chunk 312...\n",
      "[INFO] Processing chunk 313...\n",
      "[INFO] Processing chunk 314...\n",
      "[INFO] Processing chunk 315...\n",
      "[INFO] Processing chunk 316...\n",
      "[INFO] Processing chunk 317...\n",
      "[INFO] Processing chunk 318...\n",
      "[INFO] Processing chunk 319...\n",
      "[INFO] Processing chunk 320...\n",
      "[INFO] Processing chunk 321...\n",
      "[INFO] Processing chunk 322...\n",
      "[INFO] Processing chunk 323...\n",
      "[INFO] Processing chunk 324...\n",
      "[INFO] Processing chunk 325...\n",
      "[INFO] Processing chunk 326...\n",
      "[INFO] Processing chunk 327...\n",
      "[INFO] Processing chunk 328...\n",
      "[INFO] Processing chunk 329...\n",
      "[INFO] Processing chunk 330...\n",
      "[INFO] Processing chunk 331...\n",
      "[INFO] Processing chunk 332...\n",
      "[INFO] Processing chunk 333...\n",
      "[INFO] Processing chunk 334...\n",
      "[INFO] Processing chunk 335...\n",
      "[INFO] Processing chunk 336...\n",
      "[INFO] Processing chunk 337...\n",
      "[INFO] Processing chunk 338...\n",
      "[INFO] Processing chunk 339...\n",
      "[INFO] Processing chunk 340...\n",
      "[INFO] Processing chunk 341...\n",
      "[INFO] Processing chunk 342...\n",
      "[INFO] Processing chunk 343...\n",
      "[INFO] Processing chunk 344...\n",
      "[INFO] Processing chunk 345...\n",
      "[INFO] Processing chunk 346...\n",
      "[INFO] Processing chunk 347...\n",
      "[INFO] Processing chunk 348...\n",
      "[INFO] Processing chunk 349...\n",
      "[INFO] Processing chunk 350...\n",
      "[INFO] Processing chunk 351...\n",
      "[INFO] Processing chunk 352...\n",
      "[INFO] Processing chunk 353...\n",
      "[INFO] Processing chunk 354...\n",
      "[INFO] Processing chunk 355...\n",
      "[INFO] Processing chunk 356...\n",
      "[INFO] Processing chunk 357...\n",
      "[INFO] Processing chunk 358...\n",
      "[INFO] Processing chunk 359...\n",
      "[INFO] Processing chunk 360...\n",
      "[INFO] Processing chunk 361...\n",
      "[INFO] Processing chunk 362...\n",
      "[INFO] Processing chunk 363...\n",
      "[INFO] Processing chunk 364...\n",
      "[INFO] Processing chunk 365...\n",
      "[INFO] Processing chunk 366...\n",
      "[INFO] Processing chunk 367...\n",
      "[INFO] Processing chunk 368...\n",
      "[INFO] Processing chunk 369...\n",
      "[INFO] Processing chunk 370...\n",
      "[INFO] Processing chunk 371...\n",
      "[INFO] Processing chunk 372...\n",
      "[INFO] Processing chunk 373...\n",
      "[INFO] Processing chunk 374...\n",
      "[INFO] Processing chunk 375...\n",
      "[INFO] Processing chunk 376...\n",
      "[INFO] Processing chunk 377...\n",
      "[INFO] Processing chunk 378...\n",
      "[INFO] Processing chunk 379...\n",
      "[INFO] Processing chunk 380...\n",
      "[INFO] Processing chunk 381...\n",
      "[INFO] Processing chunk 382...\n",
      "[INFO] Processing chunk 383...\n",
      "[INFO] Processing chunk 384...\n",
      "[INFO] Processing chunk 385...\n",
      "[INFO] Processing chunk 386...\n",
      "[INFO] Processing chunk 387...\n",
      "[INFO] Processing chunk 388...\n",
      "[INFO] Processing chunk 389...\n",
      "[INFO] Processing chunk 390...\n",
      "[INFO] Processing chunk 391...\n",
      "[INFO] Processing chunk 392...\n",
      "[INFO] Processing chunk 393...\n",
      "[INFO] Processing chunk 394...\n",
      "[INFO] Processing chunk 395...\n",
      "[INFO] Processing chunk 396...\n",
      "[INFO] Processing chunk 397...\n",
      "[INFO] Processing chunk 398...\n",
      "[INFO] Processing chunk 399...\n",
      "[INFO] Processing chunk 400...\n",
      "[INFO] Processing chunk 401...\n",
      "[INFO] Processing chunk 402...\n",
      "[INFO] Processing chunk 403...\n",
      "[INFO] Processing chunk 404...\n",
      "[INFO] Processing chunk 405...\n",
      "[INFO] Processing chunk 406...\n",
      "[INFO] Processing chunk 407...\n",
      "[INFO] Processing chunk 408...\n",
      "[INFO] Processing chunk 409...\n",
      "[INFO] Processing chunk 410...\n",
      "[INFO] Processing chunk 411...\n",
      "[INFO] Processing chunk 412...\n",
      "[INFO] Processing chunk 413...\n",
      "[INFO] Processing chunk 414...\n",
      "[INFO] Processing chunk 415...\n",
      "[INFO] Processing chunk 416...\n",
      "[INFO] Processing chunk 417...\n",
      "[INFO] Processing chunk 418...\n",
      "[INFO] Processing chunk 419...\n",
      "[INFO] Processing chunk 420...\n",
      "[INFO] Processing chunk 421...\n",
      "[INFO] Processing chunk 422...\n",
      "[INFO] Processing chunk 423...\n",
      "[INFO] Processing chunk 424...\n",
      "[INFO] Processing chunk 425...\n",
      "[INFO] Processing chunk 426...\n",
      "[INFO] Processing chunk 427...\n",
      "[INFO] Processing chunk 428...\n",
      "[INFO] Processing chunk 429...\n",
      "[INFO] Processing chunk 430...\n",
      "[INFO] Processing chunk 431...\n",
      "[INFO] Processing chunk 432...\n",
      "[INFO] Processing chunk 433...\n",
      "[INFO] Processing chunk 434...\n",
      "[INFO] Processing chunk 435...\n",
      "[INFO] Processing chunk 436...\n",
      "[INFO] Processing chunk 437...\n",
      "[INFO] Processing chunk 438...\n",
      "[INFO] Processing chunk 439...\n",
      "[INFO] Processing chunk 440...\n",
      "[INFO] Processing chunk 441...\n",
      "[INFO] Processing chunk 442...\n",
      "[INFO] Processing chunk 443...\n",
      "[INFO] Processing chunk 444...\n",
      "[INFO] Processing chunk 445...\n",
      "[INFO] Processing chunk 446...\n",
      "[INFO] Processing chunk 447...\n",
      "[INFO] Processing chunk 448...\n",
      "[INFO] Processing chunk 449...\n",
      "[INFO] Processing chunk 450...\n",
      "[INFO] Processing chunk 451...\n",
      "[INFO] Processing chunk 452...\n",
      "[INFO] Processing chunk 453...\n",
      "[INFO] Processing chunk 454...\n",
      "[INFO] Processing chunk 455...\n",
      "[INFO] Processing chunk 456...\n",
      "[INFO] Processing chunk 457...\n",
      "[INFO] Processing chunk 458...\n",
      "[INFO] Processing chunk 459...\n",
      "[INFO] Processing chunk 460...\n",
      "[INFO] Processing chunk 461...\n",
      "[INFO] Processing chunk 462...\n",
      "[INFO] Processing chunk 463...\n",
      "[INFO] Processing chunk 464...\n",
      "[INFO] Processing chunk 465...\n",
      "[INFO] Processing chunk 466...\n",
      "[INFO] Processing chunk 467...\n",
      "[INFO] Processing chunk 468...\n",
      "[INFO] Processing chunk 469...\n",
      "[INFO] Processing chunk 470...\n",
      "[INFO] Processing chunk 471...\n",
      "[INFO] Processing chunk 472...\n",
      "[INFO] Processing chunk 473...\n",
      "[INFO] Processing chunk 474...\n",
      "[INFO] Processing chunk 475...\n",
      "[INFO] Processing chunk 476...\n",
      "[INFO] Processing chunk 477...\n",
      "[INFO] Processing chunk 478...\n",
      "[INFO] Processing chunk 479...\n",
      "[INFO] Processing chunk 480...\n",
      "[INFO] Processing chunk 481...\n",
      "[INFO] Processing chunk 482...\n",
      "[INFO] Processing chunk 483...\n",
      "[INFO] Processing chunk 484...\n",
      "[INFO] Processing chunk 485...\n",
      "[INFO] Processing chunk 486...\n",
      "[INFO] Processing chunk 487...\n",
      "[INFO] Processing chunk 488...\n",
      "[INFO] Processing chunk 489...\n",
      "[INFO] Processing chunk 490...\n",
      "[INFO] Processing chunk 491...\n",
      "[INFO] Processing chunk 492...\n",
      "[INFO] Processing chunk 493...\n",
      "[INFO] Processing chunk 494...\n",
      "[INFO] Processing chunk 495...\n",
      "[INFO] Processing chunk 496...\n",
      "[INFO] Processing chunk 497...\n",
      "[INFO] Processing chunk 498...\n",
      "[INFO] Processing chunk 499...\n",
      "[INFO] Processing chunk 500...\n",
      "[INFO] Processing chunk 501...\n",
      "[INFO] Processing chunk 502...\n",
      "[INFO] Processing chunk 503...\n",
      "[INFO] Processing chunk 504...\n",
      "[INFO] Processing chunk 505...\n",
      "[INFO] Processing chunk 506...\n",
      "[INFO] Processing chunk 507...\n",
      "[INFO] Processing chunk 508...\n",
      "[INFO] Processing chunk 509...\n",
      "[INFO] Processing chunk 510...\n",
      "[INFO] Processing chunk 511...\n",
      "[INFO] Processing chunk 512...\n",
      "[INFO] Processing chunk 513...\n",
      "[INFO] Processing chunk 514...\n",
      "[INFO] Processing chunk 515...\n",
      "[INFO] Processing chunk 516...\n",
      "[INFO] Processing chunk 517...\n",
      "[INFO] Processing chunk 518...\n",
      "[INFO] Processing chunk 519...\n",
      "[INFO] Processing chunk 520...\n",
      "[INFO] Processing chunk 521...\n",
      "[INFO] Processing chunk 522...\n",
      "[INFO] Processing chunk 523...\n",
      "[INFO] Processing chunk 524...\n",
      "[INFO] Processing chunk 525...\n",
      "[INFO] Processing chunk 526...\n",
      "[INFO] Processing chunk 527...\n",
      "[INFO] Processing chunk 528...\n",
      "[INFO] Processing chunk 529...\n",
      "[INFO] Processing chunk 530...\n",
      "[INFO] Processing chunk 531...\n",
      "[INFO] Processing chunk 532...\n",
      "[INFO] Processing chunk 533...\n",
      "[INFO] Processing chunk 534...\n",
      "[INFO] Processing chunk 535...\n",
      "[INFO] Processing chunk 536...\n",
      "[INFO] Processing chunk 537...\n",
      "[INFO] Processing chunk 538...\n",
      "[INFO] Processing chunk 539...\n",
      "[INFO] Processing chunk 540...\n",
      "[INFO] Processing chunk 541...\n",
      "[INFO] Processing chunk 542...\n",
      "[INFO] Processing chunk 543...\n",
      "[INFO] Processing chunk 544...\n",
      "[INFO] Processing chunk 545...\n",
      "[INFO] Processing chunk 546...\n",
      "[INFO] Processing chunk 547...\n",
      "[INFO] Processing chunk 548...\n",
      "[INFO] Processing chunk 549...\n",
      "[INFO] Processing chunk 550...\n",
      "[INFO] Processing chunk 551...\n",
      "[INFO] Processing chunk 552...\n",
      "[INFO] Processing chunk 553...\n",
      "[INFO] Processing chunk 554...\n",
      "[INFO] Processing chunk 555...\n",
      "[INFO] Processing chunk 556...\n",
      "[INFO] Processing chunk 557...\n",
      "[INFO] Processing chunk 558...\n",
      "[INFO] Processing chunk 559...\n",
      "[INFO] Processing chunk 560...\n",
      "[INFO] Processing chunk 561...\n",
      "[INFO] Processing chunk 562...\n",
      "[INFO] Processing chunk 563...\n",
      "[INFO] Processing chunk 564...\n",
      "[INFO] Processing chunk 565...\n",
      "[INFO] Processing chunk 566...\n",
      "[INFO] Processing chunk 567...\n",
      "[INFO] Processing chunk 568...\n",
      "[INFO] Processing chunk 569...\n",
      "[INFO] Processing chunk 570...\n",
      "[INFO] Processing chunk 571...\n",
      "[INFO] Processing chunk 572...\n",
      "[INFO] Processing chunk 573...\n",
      "[INFO] Processing chunk 574...\n",
      "[INFO] Processing chunk 575...\n",
      "[INFO] Processing chunk 576...\n",
      "[INFO] Processing chunk 577...\n",
      "[INFO] Processing chunk 578...\n",
      "[INFO] Processing chunk 579...\n",
      "[INFO] Processing chunk 580...\n",
      "[INFO] Processing chunk 581...\n",
      "[INFO] Processing chunk 582...\n",
      "[INFO] Processing chunk 583...\n",
      "[INFO] Processing chunk 584...\n",
      "[INFO] Processing chunk 585...\n",
      "[INFO] Processing chunk 586...\n",
      "[INFO] Processing chunk 587...\n",
      "[INFO] Processing chunk 588...\n",
      "[INFO] Processing chunk 589...\n",
      "[INFO] Processing chunk 590...\n",
      "[INFO] Processing chunk 591...\n",
      "[INFO] Processing chunk 592...\n",
      "[INFO] Processing chunk 593...\n",
      "[INFO] Processing chunk 594...\n",
      "[INFO] Processing chunk 595...\n",
      "[INFO] Processing chunk 596...\n",
      "[INFO] Processing chunk 597...\n",
      "[INFO] Processing chunk 598...\n",
      "[INFO] Processing chunk 599...\n",
      "[INFO] Processing chunk 600...\n",
      "[INFO] Processing chunk 601...\n",
      "[INFO] Processing chunk 602...\n",
      "[INFO] Processing chunk 603...\n",
      "[INFO] Processing chunk 604...\n",
      "[INFO] Processing chunk 605...\n",
      "[INFO] Processing chunk 606...\n",
      "[INFO] Processing chunk 607...\n",
      "[INFO] Processing chunk 608...\n",
      "[INFO] Processing chunk 609...\n",
      "[INFO] Processing chunk 610...\n",
      "[INFO] Processing chunk 611...\n",
      "[INFO] Processing chunk 612...\n",
      "[INFO] Processing chunk 613...\n",
      "[INFO] Processing chunk 614...\n",
      "[INFO] Processing chunk 615...\n",
      "[INFO] Processing chunk 616...\n",
      "[INFO] Processing chunk 617...\n",
      "[INFO] Processing chunk 618...\n",
      "[INFO] Processing chunk 619...\n",
      "[INFO] Processing chunk 620...\n",
      "[INFO] Processing chunk 621...\n",
      "[INFO] Processing chunk 622...\n",
      "[INFO] Processing chunk 623...\n",
      "[INFO] Processing chunk 624...\n",
      "[INFO] Processing chunk 625...\n",
      "[INFO] Processing chunk 626...\n",
      "[INFO] Processing chunk 627...\n",
      "[INFO] Processing chunk 628...\n",
      "[INFO] Processing chunk 629...\n",
      "[INFO] Processing chunk 630...\n",
      "[INFO] Processing chunk 631...\n",
      "[INFO] Processing chunk 632...\n",
      "[INFO] Processing chunk 633...\n",
      "[INFO] Processing chunk 634...\n",
      "[INFO] Processing chunk 635...\n",
      "[INFO] Processing chunk 636...\n",
      "[INFO] Processing chunk 637...\n",
      "[INFO] Processing chunk 638...\n",
      "[INFO] Processing chunk 639...\n",
      "[INFO] Processing chunk 640...\n",
      "[INFO] Processing chunk 641...\n",
      "[INFO] Processing chunk 642...\n",
      "[INFO] Processing chunk 643...\n",
      "[INFO] Processing chunk 644...\n",
      "[INFO] Processing chunk 645...\n",
      "[INFO] Processing chunk 646...\n",
      "[INFO] Processing chunk 647...\n",
      "[INFO] Processing chunk 648...\n",
      "[INFO] Processing chunk 649...\n",
      "[INFO] Processing chunk 650...\n",
      "[INFO] Processing chunk 651...\n",
      "[INFO] Processing chunk 652...\n",
      "[INFO] Processing chunk 653...\n",
      "[INFO] Processing chunk 654...\n",
      "[INFO] Processing chunk 655...\n",
      "[INFO] Processing chunk 656...\n",
      "[INFO] Processing chunk 657...\n",
      "[INFO] Processing chunk 658...\n",
      "[INFO] Processing chunk 659...\n",
      "[INFO] Processing chunk 660...\n",
      "[INFO] Processing chunk 661...\n",
      "[INFO] Processing chunk 662...\n",
      "[INFO] Processing chunk 663...\n",
      "[INFO] Processing chunk 664...\n",
      "[INFO] Processing chunk 665...\n",
      "[INFO] Processing chunk 666...\n",
      "[INFO] Processing chunk 667...\n",
      "[INFO] Processing chunk 668...\n",
      "[INFO] Processing chunk 669...\n",
      "[INFO] Processing chunk 670...\n",
      "[INFO] Processing chunk 671...\n",
      "[INFO] Processing chunk 672...\n",
      "[INFO] Processing chunk 673...\n",
      "[INFO] Processing chunk 674...\n",
      "[INFO] Processing chunk 675...\n",
      "[INFO] Processing chunk 676...\n",
      "[INFO] Processing chunk 677...\n",
      "[INFO] Processing chunk 678...\n",
      "[INFO] Processing chunk 679...\n",
      "[INFO] Processing chunk 680...\n",
      "[INFO] Processing chunk 681...\n",
      "[INFO] Processing chunk 682...\n",
      "[INFO] Processing chunk 683...\n",
      "[INFO] Processing chunk 684...\n",
      "[INFO] Processing chunk 685...\n",
      "[INFO] Processing chunk 686...\n",
      "[INFO] Processing chunk 687...\n",
      "[INFO] Processing chunk 688...\n",
      "[INFO] Processing chunk 689...\n",
      "[INFO] Processing chunk 690...\n",
      "[INFO] Processing chunk 691...\n",
      "[INFO] Processing chunk 692...\n",
      "[INFO] Processing chunk 693...\n",
      "[INFO] Processing chunk 694...\n",
      "[INFO] Processing chunk 695...\n",
      "[INFO] Processing chunk 696...\n",
      "[INFO] Processing chunk 697...\n",
      "[INFO] Processing chunk 698...\n",
      "[INFO] Processing chunk 699...\n",
      "[INFO] Processing chunk 700...\n",
      "[INFO] Processing chunk 701...\n",
      "[INFO] Processing chunk 702...\n",
      "[INFO] Processing chunk 703...\n",
      "[INFO] Processing chunk 704...\n",
      "[INFO] Processing chunk 705...\n",
      "[INFO] Processing chunk 706...\n",
      "[INFO] Processing chunk 707...\n",
      "[INFO] Processing chunk 708...\n",
      "[INFO] Processing chunk 709...\n",
      "[INFO] Processing chunk 710...\n",
      "[INFO] Processing chunk 711...\n",
      "[INFO] Processing chunk 712...\n",
      "[INFO] Processing chunk 713...\n",
      "[INFO] Processing chunk 714...\n",
      "[INFO] Processing chunk 715...\n",
      "[INFO] Processing chunk 716...\n",
      "[INFO] Processing chunk 717...\n",
      "[INFO] Processing chunk 718...\n",
      "[INFO] Processing chunk 719...\n",
      "[INFO] Processing chunk 720...\n",
      "[INFO] Processing chunk 721...\n",
      "[INFO] Processing chunk 722...\n",
      "[INFO] Processing chunk 723...\n",
      "[INFO] Processing chunk 724...\n",
      "[INFO] Processing chunk 725...\n",
      "[INFO] Processing chunk 726...\n",
      "[INFO] Processing chunk 727...\n",
      "[INFO] Processing chunk 728...\n",
      "[INFO] Processing chunk 729...\n",
      "[INFO] Processing chunk 730...\n",
      "[INFO] Processing chunk 731...\n",
      "[INFO] Processing chunk 732...\n",
      "[INFO] Processing chunk 733...\n",
      "[INFO] Processing chunk 734...\n",
      "[INFO] Processing chunk 735...\n",
      "[INFO] Processing chunk 736...\n",
      "[INFO] Processing chunk 737...\n",
      "[INFO] Processing chunk 738...\n",
      "[INFO] Processing chunk 739...\n",
      "[INFO] Processing chunk 740...\n",
      "[INFO] Processing chunk 741...\n",
      "[INFO] Processing chunk 742...\n",
      "[INFO] Processing chunk 743...\n",
      "[INFO] Processing chunk 744...\n",
      "[INFO] Processing chunk 745...\n",
      "[INFO] Processing chunk 746...\n",
      "[INFO] Processing chunk 747...\n",
      "[INFO] Processing chunk 748...\n",
      "[INFO] Processing chunk 749...\n",
      "[INFO] Processing chunk 750...\n",
      "[INFO] Processing chunk 751...\n",
      "[INFO] Processing chunk 752...\n",
      "[INFO] Processing chunk 753...\n",
      "[INFO] Processing chunk 754...\n",
      "[INFO] Processing chunk 755...\n",
      "[INFO] Processing chunk 756...\n",
      "[INFO] Processing chunk 757...\n",
      "[INFO] Processing chunk 758...\n",
      "[INFO] Processing chunk 759...\n",
      "[INFO] Processing chunk 760...\n",
      "[INFO] Processing chunk 761...\n",
      "[INFO] Processing chunk 762...\n",
      "[INFO] Processing chunk 763...\n",
      "[INFO] Processing chunk 764...\n",
      "[INFO] Processing chunk 765...\n",
      "[INFO] Processing chunk 766...\n",
      "[INFO] Processing chunk 767...\n",
      "[INFO] Processing chunk 768...\n",
      "[INFO] Processing chunk 769...\n",
      "[INFO] Processing chunk 770...\n",
      "[INFO] Processing chunk 771...\n",
      "[INFO] Processing chunk 772...\n",
      "[INFO] Processing chunk 773...\n",
      "[INFO] Processing chunk 774...\n",
      "[INFO] Processing chunk 775...\n",
      "[INFO] Processing chunk 776...\n",
      "[INFO] Processing chunk 777...\n",
      "[INFO] Processing chunk 778...\n",
      "[INFO] Processing chunk 779...\n",
      "[INFO] Processing chunk 780...\n",
      "[INFO] Processing chunk 781...\n",
      "[INFO] Processing chunk 782...\n",
      "[INFO] Processing chunk 783...\n",
      "[INFO] Processing chunk 784...\n",
      "[INFO] Processing chunk 785...\n",
      "[INFO] Processing chunk 786...\n",
      "[INFO] Processing chunk 787...\n",
      "[INFO] Processing chunk 788...\n",
      "[INFO] Processing chunk 789...\n",
      "[INFO] Processing chunk 790...\n",
      "[INFO] Processing chunk 791...\n",
      "[INFO] Processing chunk 792...\n",
      "[INFO] Processing chunk 793...\n",
      "[INFO] Processing chunk 794...\n",
      "[INFO] Processing chunk 795...\n",
      "[INFO] Processing chunk 796...\n",
      "[INFO] Processing chunk 797...\n",
      "[INFO] Processing chunk 798...\n",
      "[INFO] Processing chunk 799...\n",
      "[INFO] Processing chunk 800...\n",
      "[INFO] Processing chunk 801...\n",
      "[INFO] Processing chunk 802...\n",
      "[INFO] Processing chunk 803...\n",
      "[INFO] Processing chunk 804...\n",
      "[INFO] Processing chunk 805...\n",
      "[INFO] Processing chunk 806...\n",
      "[INFO] Processing chunk 807...\n",
      "[INFO] Processing chunk 808...\n",
      "[INFO] Processing chunk 809...\n",
      "[INFO] Processing chunk 810...\n",
      "[INFO] Processing chunk 811...\n",
      "[INFO] Processing chunk 812...\n",
      "[INFO] Processing chunk 813...\n",
      "[INFO] Processing chunk 814...\n",
      "[INFO] Processing chunk 815...\n",
      "[INFO] Processing chunk 816...\n",
      "[INFO] Processing chunk 817...\n",
      "[INFO] Processing chunk 818...\n",
      "[INFO] Processing chunk 819...\n",
      "[INFO] Processing chunk 820...\n",
      "[INFO] Processing chunk 821...\n",
      "[INFO] Processing chunk 822...\n",
      "[INFO] Processing chunk 823...\n",
      "[INFO] Processing chunk 824...\n",
      "[INFO] Processing chunk 825...\n",
      "[INFO] Processing chunk 826...\n",
      "[INFO] Processing chunk 827...\n",
      "[INFO] Processing chunk 828...\n",
      "[INFO] Processing chunk 829...\n",
      "[INFO] Processing chunk 830...\n",
      "[INFO] Processing chunk 831...\n",
      "[INFO] Processing chunk 832...\n",
      "[INFO] Processing chunk 833...\n",
      "[INFO] Processing chunk 834...\n",
      "[INFO] Processing chunk 835...\n",
      "[INFO] Processing chunk 836...\n",
      "[INFO] Processing chunk 837...\n",
      "[INFO] Processing chunk 838...\n",
      "[INFO] Processing chunk 839...\n",
      "[INFO] Processing chunk 840...\n",
      "[INFO] Processing chunk 841...\n",
      "[INFO] Processing chunk 842...\n",
      "[INFO] Processing chunk 843...\n",
      "[INFO] Processing chunk 844...\n",
      "[INFO] Processing chunk 845...\n",
      "[INFO] Processing chunk 846...\n",
      "[INFO] Processing chunk 847...\n",
      "[INFO] Processing chunk 848...\n",
      "[INFO] Processing chunk 849...\n",
      "[INFO] Processing chunk 850...\n",
      "[INFO] Processing chunk 851...\n",
      "[INFO] Processing chunk 852...\n",
      "[INFO] Processing chunk 853...\n",
      "[INFO] Processing chunk 854...\n",
      "[INFO] Processing chunk 855...\n",
      "[INFO] Processing chunk 856...\n",
      "[INFO] Processing chunk 857...\n",
      "[INFO] Processing chunk 858...\n",
      "[INFO] Processing chunk 859...\n",
      "[INFO] Processing chunk 860...\n",
      "[INFO] Processing chunk 861...\n",
      "[INFO] Processing chunk 862...\n",
      "[INFO] Processing chunk 863...\n",
      "[INFO] Processing chunk 864...\n",
      "[INFO] Processing chunk 865...\n",
      "[INFO] Processing chunk 866...\n",
      "[INFO] Processing chunk 867...\n",
      "[INFO] Processing chunk 868...\n",
      "[INFO] Processing chunk 869...\n",
      "[INFO] Processing chunk 870...\n",
      "[INFO] Processing chunk 871...\n",
      "[INFO] Processing chunk 872...\n",
      "[INFO] Processing chunk 873...\n",
      "[INFO] Processing chunk 874...\n",
      "[INFO] Processing chunk 875...\n",
      "[INFO] Processing chunk 876...\n",
      "[INFO] Processing chunk 877...\n",
      "[INFO] Processing chunk 878...\n",
      "[INFO] Processing chunk 879...\n",
      "[INFO] Processing chunk 880...\n",
      "[INFO] Processing chunk 881...\n",
      "[INFO] Processing chunk 882...\n",
      "[INFO] Processing chunk 883...\n",
      "[INFO] Processing chunk 884...\n",
      "[INFO] Processing chunk 885...\n",
      "[INFO] Processing chunk 886...\n",
      "[INFO] Processing chunk 887...\n",
      "[INFO] Processing chunk 888...\n",
      "[INFO] Processing chunk 889...\n",
      "[INFO] Processing chunk 890...\n",
      "[INFO] Processing chunk 891...\n",
      "[INFO] Processing chunk 892...\n",
      "[INFO] Processing chunk 893...\n",
      "[INFO] Processing chunk 894...\n",
      "[INFO] Processing chunk 895...\n",
      "[INFO] Processing chunk 896...\n",
      "[INFO] Processing chunk 897...\n",
      "[INFO] Processing chunk 898...\n",
      "[INFO] Processing chunk 899...\n",
      "[INFO] Processing chunk 900...\n",
      "[INFO] Processing chunk 901...\n",
      "[INFO] Processing chunk 902...\n",
      "[INFO] Processing chunk 903...\n",
      "[INFO] Processing chunk 904...\n",
      "[INFO] Processing chunk 905...\n",
      "[INFO] Processing chunk 906...\n",
      "[INFO] Processing chunk 907...\n",
      "[INFO] Processing chunk 908...\n",
      "[INFO] Processing chunk 909...\n",
      "[INFO] Processing chunk 910...\n",
      "[INFO] Processing chunk 911...\n",
      "[INFO] Processing chunk 912...\n",
      "[INFO] Processing chunk 913...\n",
      "[INFO] Processing chunk 914...\n",
      "[INFO] Processing chunk 915...\n",
      "[INFO] Processing chunk 916...\n",
      "[INFO] Processing chunk 917...\n",
      "[INFO] Processing chunk 918...\n",
      "[INFO] Processing chunk 919...\n",
      "[INFO] Processing chunk 920...\n",
      "[INFO] Processing chunk 921...\n",
      "[INFO] Processing chunk 922...\n",
      "[INFO] Processing chunk 923...\n",
      "[INFO] Processing chunk 924...\n",
      "[INFO] Processing chunk 925...\n",
      "[INFO] Processing chunk 926...\n",
      "[INFO] Processing chunk 927...\n",
      "[INFO] Processing chunk 928...\n",
      "[INFO] Processing chunk 929...\n",
      "[INFO] Processing chunk 930...\n",
      "[INFO] Processing chunk 931...\n",
      "[INFO] Processing chunk 932...\n",
      "[INFO] Processing chunk 933...\n",
      "[INFO] Processing chunk 934...\n",
      "[INFO] Processing chunk 935...\n",
      "[INFO] Processing chunk 936...\n",
      "[INFO] Processing chunk 937...\n",
      "[INFO] Processing chunk 938...\n",
      "[INFO] Processing chunk 939...\n",
      "[INFO] Processing chunk 940...\n",
      "[INFO] Processing chunk 941...\n",
      "[INFO] Processing chunk 942...\n",
      "[INFO] Processing chunk 943...\n",
      "[INFO] Processing chunk 944...\n",
      "[INFO] Processing chunk 945...\n",
      "[INFO] Processing chunk 946...\n",
      "[INFO] Processing chunk 947...\n",
      "[INFO] Processing chunk 948...\n",
      "[INFO] Processing chunk 949...\n",
      "[INFO] Processing chunk 950...\n",
      "[INFO] Processing chunk 951...\n",
      "[INFO] Processing chunk 952...\n",
      "[INFO] Processing chunk 953...\n",
      "[INFO] Processing chunk 954...\n",
      "[INFO] Processing chunk 955...\n",
      "[INFO] Processing chunk 956...\n",
      "[INFO] Processing chunk 957...\n",
      "[INFO] Processing chunk 958...\n",
      "[INFO] Processing chunk 959...\n",
      "[INFO] Processing chunk 960...\n",
      "[INFO] Processing chunk 961...\n",
      "[INFO] Processing chunk 962...\n",
      "[INFO] Processing chunk 963...\n",
      "[INFO] Processing chunk 964...\n",
      "[INFO] Processing chunk 965...\n",
      "[INFO] Processing chunk 966...\n",
      "[INFO] Processing chunk 967...\n",
      "[INFO] Processing chunk 968...\n",
      "[INFO] Processing chunk 969...\n",
      "[INFO] Processing chunk 970...\n",
      "[INFO] Processing chunk 971...\n",
      "[INFO] Processing chunk 972...\n",
      "[INFO] Processing chunk 973...\n",
      "[INFO] Processing chunk 974...\n",
      "[INFO] Processing chunk 975...\n",
      "[INFO] Processing chunk 976...\n",
      "[INFO] Processing chunk 977...\n",
      "[INFO] Processing chunk 978...\n",
      "[INFO] Processing chunk 979...\n",
      "[INFO] Processing chunk 980...\n",
      "[INFO] Processing chunk 981...\n",
      "[INFO] Processing chunk 982...\n",
      "[INFO] Processing chunk 983...\n",
      "[INFO] Processing chunk 984...\n",
      "[INFO] Processing chunk 985...\n",
      "[INFO] Processing chunk 986...\n",
      "[INFO] Processing chunk 987...\n",
      "[INFO] Processing chunk 988...\n",
      "[INFO] Processing chunk 989...\n",
      "[INFO] Processing chunk 990...\n",
      "[INFO] Processing chunk 991...\n",
      "[INFO] Processing chunk 992...\n",
      "[INFO] Processing chunk 993...\n",
      "[INFO] Processing chunk 994...\n",
      "[INFO] Processing chunk 995...\n",
      "[INFO] Processing chunk 996...\n",
      "[INFO] Processing chunk 997...\n",
      "[INFO] Processing chunk 998...\n",
      "[INFO] Processing chunk 999...\n",
      "[INFO] Processing chunk 1000...\n",
      "[INFO] Updated dataset saved successfully as 'updated_relation_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Paths\n",
    "input_filename = \"relation_dataset.csv\"\n",
    "output_filename = \"updated_relation_dataset.csv\"\n",
    "\n",
    "# Chunk size for processing in batches\n",
    "chunk_size = 100  # Number of rows per chunk\n",
    "\n",
    "# Function to filter and simplify the relations\n",
    "def simplify_relations(relations_json):\n",
    "    \"\"\"\n",
    "    Simplifies the relations by keeping only relevant fields and removing duplicates.\n",
    "\n",
    "    Parameters:\n",
    "        relations_json (str): JSON string of relations.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Simplified list of relations with only 'start', 'end', and 'weight'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relations = json.loads(relations_json)  # Parse the JSON string\n",
    "        unique_relations = {}\n",
    "        for rel in relations:\n",
    "            key = (rel.get(\"start\", {}).get(\"label\", \"unknown\"),\n",
    "                   rel.get(\"end\", {}).get(\"label\", \"unknown\"),\n",
    "                   rel.get(\"weight\", 0))\n",
    "            unique_relations[key] = {\"start\": key[0], \"end\": key[1], \"weight\": key[2]}\n",
    "        return json.dumps(list(unique_relations.values()))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return \"[]\"  # Return empty list if parsing fails\n",
    "\n",
    "# Process dataset in chunks with joblib\n",
    "def process_in_parallel_joblib(input_file, output_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Processes the input dataset in parallel using joblib and writes the updated dataset incrementally.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "        chunk_size (int): Number of rows per chunk.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Processing dataset in chunks of {chunk_size} rows...\")\n",
    "\n",
    "    # Remove existing output file if exists\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    is_header_written = False  # Flag to track header writing\n",
    "\n",
    "    with pd.read_csv(input_file, chunksize=chunk_size) as reader:\n",
    "        for chunk_idx, chunk in enumerate(reader, start=1):\n",
    "            print(f\"[INFO] Processing chunk {chunk_idx}...\")\n",
    "\n",
    "            if \"relations\" not in chunk.columns:\n",
    "                raise ValueError(\"[ERROR] 'relations' column not found in the dataset.\")\n",
    "\n",
    "            # Parallel processing with joblib\n",
    "            chunk[\"relations\"] = Parallel(n_jobs=-1)(\n",
    "                delayed(simplify_relations)(relations) for relations in chunk[\"relations\"]\n",
    "            )\n",
    "\n",
    "            # Write the processed chunk incrementally\n",
    "            chunk.to_csv(output_file, index=False, mode=\"a\", header=not is_header_written)\n",
    "            is_header_written = True  # Ensure headers are written only once\n",
    "\n",
    "    print(f\"[INFO] Updated dataset saved successfully as '{output_file}'.\")\n",
    "\n",
    "# Run the function in Jupyter\n",
    "process_in_parallel_joblib(input_filename, output_filename, chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a2665-fcc7-4570-899c-921b0443770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Condense the dataset to around 100,000 rows with equal distribution across the 'type' column\n",
    "# desired_rows_per_type = 25000  # Approximate number of rows per type\n",
    "# grouped = dataset.groupby('type')  # Group the dataset by the 'type' column\n",
    "\n",
    "# # Sample rows equally from each type\n",
    "# condensed_dataset = grouped.apply(\n",
    "#     lambda x: x.sample(n=min(len(x), desired_rows_per_type), random_state=42)\n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "# print(\"[INFO] Condensed dataset created with equal distribution across types.\")\n",
    "\n",
    "# # Save the condensed dataset to a CSV file\n",
    "# output_filename = \"relation_dataset.csv\"\n",
    "# condensed_dataset.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "# print(f\"[INFO] Condensed dataset saved successfully as '{output_filename}'.\")\n",
    "\n",
    "# # Update the dataset variable to the condensed dataset\n",
    "# dataset = condensed_dataset\n",
    "\n",
    "# # Print the first few rows of the condensed dataset\n",
    "# print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa70c5-8b02-4caa-84fc-1cd51e678d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4139f5c2-28c8-4941-845e-ff6176f6e42c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pre-converted Numberbatch embeddings found at ./numberbatch-en-19.08.npy.\n",
      "\n",
      "[INFO] Loading the updated relation dataset...\n",
      "[INFO] Dataset loaded successfully. Total rows: 100000\n",
      "\n",
      "[INFO] Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       100000 non-null  object\n",
      " 1   label      100000 non-null  int64 \n",
      " 2   type       100000 non-null  object\n",
      " 3   concepts   100000 non-null  object\n",
      " 4   relations  100000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 3.8+ MB\n",
      "None\n",
      "\n",
      "[INFO] Sample Rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>concepts</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good he answer ok allow trial worker.</td>\n",
       "      <td>0</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['trial worker', 'he']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I met the world's tallest and the world's heav...</td>\n",
       "      <td>1</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['i', 'the other day']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was at a barber shop in Bangkok and I asked ...</td>\n",
       "      <td>1</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['rainbow swirls', 'me,\"well', 'bangkok', 'the...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Its build card time factor.</td>\n",
       "      <td>0</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['its build card time factor']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A cashier asks a French guy would you like a b...</td>\n",
       "      <td>1</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['french', 'the french guy', 'this year', 'a c...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label             type  \\\n",
       "0              Good he answer ok allow trial worker.      0  body punchlines   \n",
       "1  I met the world's tallest and the world's heav...      1  body punchlines   \n",
       "2  I was at a barber shop in Bangkok and I asked ...      1  body punchlines   \n",
       "3                        Its build card time factor.      0  body punchlines   \n",
       "4  A cashier asks a French guy would you like a b...      1  body punchlines   \n",
       "\n",
       "                                            concepts relations  \n",
       "0                             ['trial worker', 'he']        []  \n",
       "1                             ['i', 'the other day']        []  \n",
       "2  ['rainbow swirls', 'me,\"well', 'bangkok', 'the...        []  \n",
       "3                     ['its build card time factor']        []  \n",
       "4  ['french', 'the french guy', 'this year', 'a c...        []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loading the cleaned concept cache...\n",
      "[INFO] Concept cache loaded successfully. Total concepts cached: 46\n",
      "\n",
      "[INFO] Sample Concept Cache Entries:\n",
      "Concept: 9\n",
      "Relations: [{'start': 'cardinal', 'end': '9', 'relation': 'wn', 'surfaceText': '[[cardinal]] is similar to [[9]]', 'weight': 2.0}, {'start': '9', 'end': 'a number', 'relation': 'number', 'surfaceText': '[[9]] is [[a number]]', 'weight': 2.0}, {'start': 'nine', 'end': '9', 'relation': 'wn', 'surfaceText': '[[nine]] is a synonym of [[9]]', 'weight': 2.0}, {'start': '9', 'end': 'nine', 'relation': 'quantity', 'surfaceText': '[[9]] is a synonym of [[nine]]', 'weight': 2.0}, {'start': 'ix', 'end': '9', 'relation': 'wn', 'surfaceText': '[[ix]] is a synonym of [[9]]', 'weight': 2.0}]\n",
      "\n",
      "Concept: 1\n",
      "Relations: [{'start': '1', 'end': 'a number', 'relation': 'number', 'surfaceText': '[[1]] is [[a number]]', 'weight': 4.47213595499958}, {'start': '1s', 'end': '1', 'relation': '1', 'surfaceText': None, 'weight': 2.0}, {'start': '1', 'end': 'a1', 'relation': 'a1', 'surfaceText': None, 'weight': 2.0}, {'start': '1', 'end': 'cardinal', 'relation': 'wn', 'surfaceText': '[[1]] is similar to [[cardinal]]', 'weight': 2.0}, {'start': '1', 'end': 'Abstaction', 'relation': 'abstaction', 'surfaceText': '[[1]] is a kind of [[Abstaction]].', 'weight': 2.0}]\n",
      "\n",
      "\n",
      "[INFO] Loading pre-converted Numberbatch embeddings...\n",
      "[INFO] Embeddings loaded successfully. Total embeddings: 516782\n",
      "\n",
      "[INFO] Extracting all unique relation types...\n",
      "[INFO] Total unique relation types: 56\n",
      "[INFO] Sample relation types: ['0', '1', '5', '6', 'a', 'a1', 'abstaction', 'alphabet', 'apprentice', 'archaic']\n",
      "\n",
      "[INFO] Sampling 10,000 rows per 'type' to create a balanced subset...\n",
      "[INFO] Unique types found: ['body punchlines' 'news headlines' 'puns' 'storylines']\n",
      "[INFO] Sampled 10000 rows for type 'body punchlines'.\n",
      "[INFO] Sampled 10000 rows for type 'news headlines'.\n",
      "[INFO] Sampled 10000 rows for type 'puns'.\n",
      "[INFO] Sampled 10000 rows for type 'storylines'.\n",
      "[INFO] Total sampled rows: 40000\n",
      "\n",
      "[INFO] Sampled Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       40000 non-null  object\n",
      " 1   label      40000 non-null  int64 \n",
      " 2   type       40000 non-null  object\n",
      " 3   concepts   40000 non-null  object\n",
      " 4   relations  40000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "\n",
      "[INFO] Sampled Dataset Sample Rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>concepts</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whats red and white and sits in a tree? A sani...</td>\n",
       "      <td>1</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['what', 'a sanitary owl', 'a tree']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discover idea traditional like another reach t...</td>\n",
       "      <td>0</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['another', 'discover idea', 'task treatment']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seat center spend place.</td>\n",
       "      <td>0</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['seat center', 'place']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Place rather about garden reality local can in...</td>\n",
       "      <td>0</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['garden reality']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I ve been cleaning my shoes with Vodka removed</td>\n",
       "      <td>1</td>\n",
       "      <td>body punchlines</td>\n",
       "      <td>['i', 'vodka', 'my shoes']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label             type  \\\n",
       "0  Whats red and white and sits in a tree? A sani...      1  body punchlines   \n",
       "1  Discover idea traditional like another reach t...      0  body punchlines   \n",
       "2                           Seat center spend place.      0  body punchlines   \n",
       "3  Place rather about garden reality local can in...      0  body punchlines   \n",
       "4     I ve been cleaning my shoes with Vodka removed      1  body punchlines   \n",
       "\n",
       "                                         concepts relations  \n",
       "0            ['what', 'a sanitary owl', 'a tree']        []  \n",
       "1  ['another', 'discover idea', 'task treatment']        []  \n",
       "2                        ['seat center', 'place']        []  \n",
       "3                              ['garden reality']        []  \n",
       "4                      ['i', 'vodka', 'my shoes']        []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Parsing 'concepts' column from string to list...\n",
      "[INFO] Parsing completed.\n",
      "\n",
      "[INFO] Aggregating ConceptNet embeddings for each row...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2494310f6c246c69f683664469b4929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating Embeddings:   0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manav/miniconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ensure that tqdm works well in Jupyter\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define file paths\n",
    "UPDATED_RELATION_DATASET_PATH = \"./data/updated_relation_dataset.csv\"\n",
    "NUMBERBATCH_TXT_PATH = \"./numberbatch-en-19.08.txt\"  # Path to your specific Numberbatch file\n",
    "NUMBERBATCH_NPY_PATH = \"./numberbatch-en-19.08.npy\"  # Path to save the converted .npy file\n",
    "CLEANED_CONCEPT_CACHE_PATH = \"./cleaned_concept_cache.json\"\n",
    "FINAL_PARQUET_PATH = \"./final_dataset_with_embeddings.parquet\"\n",
    "\n",
    "# Define embedding dimension\n",
    "EMBED_DIM = 300\n",
    "\n",
    "# Step 1: Convert Numberbatch Embeddings to NumPy Format\n",
    "def convert_numberbatch_to_npy(txt_file_path, output_npy_path, embed_dim=300):\n",
    "    \"\"\"\n",
    "    Converts ConceptNet Numberbatch embeddings from text format to NumPy format.\n",
    "\n",
    "    Parameters:\n",
    "        txt_file_path (str): Path to the `numberbatch-en-19.08.txt` file.\n",
    "        output_npy_path (str): Path to save the `numberbatch-en-19.08.npy` file.\n",
    "        embed_dim (int): Dimension of the embeddings.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Converting Numberbatch embeddings from text to NumPy format...\")\n",
    "    embeddings_dict = {}\n",
    "    with open(txt_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(tqdm(f, desc=\"Converting embeddings\")):\n",
    "            if idx == 0:  # Skip the first line (header)\n",
    "                continue\n",
    "            values = line.strip().split()\n",
    "            if len(values) != embed_dim + 1:\n",
    "                continue  # Skip lines that don't have the correct number of dimensions\n",
    "            word = values[0].lower()\n",
    "            try:\n",
    "                vector = np.array(values[1:], dtype=np.float32)\n",
    "                embeddings_dict[word] = vector\n",
    "            except ValueError:\n",
    "                # Skip lines with invalid float values\n",
    "                continue\n",
    "            if idx % 100000 == 0 and idx != 0:\n",
    "                print(f\"[INFO] Processed {idx} embeddings...\")\n",
    "    np.save(output_npy_path, embeddings_dict)\n",
    "    print(f\"[INFO] Embeddings saved to {output_npy_path}.\")\n",
    "\n",
    "# Check if .npy file exists; if not, convert the text file\n",
    "if not os.path.exists(NUMBERBATCH_NPY_PATH):\n",
    "    if os.path.exists(NUMBERBATCH_TXT_PATH):\n",
    "        convert_numberbatch_to_npy(NUMBERBATCH_TXT_PATH, NUMBERBATCH_NPY_PATH, embed_dim=EMBED_DIM)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"[ERROR] Numberbatch text file not found at {NUMBERBATCH_TXT_PATH}\")\n",
    "else:\n",
    "    print(f\"[INFO] Pre-converted Numberbatch embeddings found at {NUMBERBATCH_NPY_PATH}.\")\n",
    "\n",
    "# Step 2: Load the Updated Relation Dataset\n",
    "print(\"\\n[INFO] Loading the updated relation dataset...\")\n",
    "if os.path.exists(UPDATED_RELATION_DATASET_PATH):\n",
    "    dataset = pd.read_csv(UPDATED_RELATION_DATASET_PATH)\n",
    "    print(f\"[INFO] Dataset loaded successfully. Total rows: {len(dataset)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"[ERROR] Dataset file not found at {UPDATED_RELATION_DATASET_PATH}\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\n[INFO] Dataset Info:\")\n",
    "print(dataset.info())\n",
    "\n",
    "print(\"\\n[INFO] Sample Rows:\")\n",
    "display(dataset.head())\n",
    "\n",
    "# Step 3: Load the Cleaned Concept Cache\n",
    "print(\"\\n[INFO] Loading the cleaned concept cache...\")\n",
    "if os.path.exists(CLEANED_CONCEPT_CACHE_PATH):\n",
    "    with open(CLEANED_CONCEPT_CACHE_PATH, 'r', encoding='utf-8') as f:\n",
    "        concept_cache = json.load(f)\n",
    "    print(f\"[INFO] Concept cache loaded successfully. Total concepts cached: {len(concept_cache)}\")\n",
    "    \n",
    "    # Verify the structure of the concept_cache\n",
    "    print(\"\\n[INFO] Sample Concept Cache Entries:\")\n",
    "    sample_concepts = list(concept_cache.keys())[:2]\n",
    "    for concept in sample_concepts:\n",
    "        print(f\"Concept: {concept}\")\n",
    "        print(f\"Relations: {concept_cache[concept]}\\n\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"[ERROR] Concept cache file not found at {CLEANED_CONCEPT_CACHE_PATH}\")\n",
    "\n",
    "# Step 4: Load the Pre-converted Numberbatch Embeddings\n",
    "print(\"\\n[INFO] Loading pre-converted Numberbatch embeddings...\")\n",
    "if os.path.exists(NUMBERBATCH_NPY_PATH):\n",
    "    embeddings_dict = np.load(NUMBERBATCH_NPY_PATH, allow_pickle=True).item()\n",
    "    print(f\"[INFO] Embeddings loaded successfully. Total embeddings: {len(embeddings_dict)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"[ERROR] Embeddings file not found at {NUMBERBATCH_NPY_PATH}\")\n",
    "\n",
    "# Step 5: Define Functions for Aggregation\n",
    "\n",
    "def parse_concepts(concepts_str):\n",
    "    \"\"\"\n",
    "    Parses the 'concepts' column from string to list.\n",
    "\n",
    "    Parameters:\n",
    "        concepts_str (str): String representation of concepts.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of concepts.\n",
    "    \"\"\"\n",
    "    if isinstance(concepts_str, str):\n",
    "        try:\n",
    "            # Replace single quotes with double quotes for valid JSON\n",
    "            concepts = json.loads(concepts_str.replace(\"'\", '\"'))\n",
    "            return concepts\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: split by comma\n",
    "            return [item.strip() for item in concepts_str.split(',')]\n",
    "    elif isinstance(concepts_str, list):\n",
    "        return concepts_str\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def aggregate_embeddings(concepts, embeddings_dict, embed_dim=300):\n",
    "    \"\"\"\n",
    "    Aggregates embeddings for a list of concepts by computing their mean.\n",
    "\n",
    "    Parameters:\n",
    "        concepts (List[str]): List of concepts.\n",
    "        embeddings_dict (dict): Preloaded embeddings dictionary.\n",
    "        embed_dim (int): Dimension of the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Aggregated embedding vector.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for concept in concepts:\n",
    "        # Normalize the concept key\n",
    "        key = concept.lower().replace(\" \", \"_\")\n",
    "        vector = embeddings_dict.get(key)\n",
    "        if vector is not None:\n",
    "            vectors.append(vector)\n",
    "    if vectors:\n",
    "        aggregated = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        aggregated = np.zeros(embed_dim, dtype=np.float32)\n",
    "    return aggregated\n",
    "\n",
    "def get_all_relation_types(relations_dict):\n",
    "    \"\"\"\n",
    "    Extracts all unique relation types from the relations dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        relations_dict (dict): Dictionary mapping concepts to their relations.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of unique relation types.\n",
    "    \"\"\"\n",
    "    relation_types = set()\n",
    "    for relations in relations_dict.values():\n",
    "        for relation in relations:\n",
    "            if isinstance(relation, dict):\n",
    "                rel_type = relation.get('relation')\n",
    "            elif isinstance(relation, (list, tuple)) and len(relation) > 0:\n",
    "                rel_type = relation[0]  # Assuming first element is the type\n",
    "            else:\n",
    "                rel_type = None\n",
    "            if rel_type:\n",
    "                relation_types.add(rel_type)\n",
    "    return sorted(list(relation_types))\n",
    "\n",
    "def aggregate_relations(relations_json, relation_types):\n",
    "    \"\"\"\n",
    "    Aggregates relation counts for a list of relations.\n",
    "\n",
    "    Parameters:\n",
    "        relations_json (str): JSON string of relations.\n",
    "        relation_types (List[str]): List of all possible relation types.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of counts corresponding to each relation type.\n",
    "    \"\"\"\n",
    "    relation_counts = defaultdict(int)\n",
    "    try:\n",
    "        relations = json.loads(relations_json)\n",
    "        for rel in relations:\n",
    "            if isinstance(rel, dict):\n",
    "                relation_type = rel.get('relation')\n",
    "            elif isinstance(rel, (list, tuple)) and len(rel) > 0:\n",
    "                relation_type = rel[0]  # Assuming first element is the type\n",
    "            else:\n",
    "                relation_type = None\n",
    "            if relation_type in relation_types:\n",
    "                relation_counts[relation_type] += 1\n",
    "    except json.JSONDecodeError:\n",
    "        pass  # Return counts as zero if JSON is invalid\n",
    "    # Create a count vector\n",
    "    counts = [relation_counts[rel] for rel in relation_types]\n",
    "    return np.array(counts, dtype=np.float32)\n",
    "\n",
    "# Step 6: Extract All Unique Relation Types\n",
    "print(\"\\n[INFO] Extracting all unique relation types...\")\n",
    "unique_relation_types = get_all_relation_types(concept_cache)\n",
    "print(f\"[INFO] Total unique relation types: {len(unique_relation_types)}\")\n",
    "print(f\"[INFO] Sample relation types: {unique_relation_types[:10]}\")\n",
    "\n",
    "# If no relation types found, inspect the concept_cache\n",
    "if len(unique_relation_types) == 0:\n",
    "    print(\"\\n[WARNING] No relation types found. Inspecting the structure of concept_cache.json...\")\n",
    "    for concept, relations in list(concept_cache.items())[:2]:\n",
    "        print(f\"Concept: {concept}\")\n",
    "        print(f\"Relations: {relations}\\n\")\n",
    "    print(\"[INFO] Proceeding without aggregating relation features.\")\n",
    "    # Proceed without relation features\n",
    "\n",
    "# Step 7: Sample 10,000 Rows per 'type'\n",
    "print(\"\\n[INFO] Sampling 10,000 rows per 'type' to create a balanced subset...\")\n",
    "desired_rows_per_type = 10000\n",
    "\n",
    "# Get unique types\n",
    "unique_types = dataset['type'].unique()\n",
    "print(f\"[INFO] Unique types found: {unique_types}\")\n",
    "\n",
    "# Initialize an empty DataFrame for the sampled data\n",
    "sampled_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterate over each type and sample rows\n",
    "for t in unique_types:\n",
    "    type_subset = dataset[dataset['type'] == t]\n",
    "    n_samples = min(desired_rows_per_type, len(type_subset))\n",
    "    sampled_subset = type_subset.sample(n=n_samples, random_state=42)\n",
    "    sampled_dataset = pd.concat([sampled_dataset, sampled_subset], ignore_index=True)\n",
    "    print(f\"[INFO] Sampled {n_samples} rows for type '{t}'.\")\n",
    "\n",
    "print(f\"[INFO] Total sampled rows: {len(sampled_dataset)}\")\n",
    "\n",
    "# Replace the original dataset with the sampled dataset for further processing\n",
    "dataset = sampled_dataset\n",
    "\n",
    "# Display information about the sampled dataset\n",
    "print(\"\\n[INFO] Sampled Dataset Info:\")\n",
    "print(dataset.info())\n",
    "\n",
    "print(\"\\n[INFO] Sampled Dataset Sample Rows:\")\n",
    "display(dataset.head())\n",
    "\n",
    "# Step 8: Parse the 'concepts' Column\n",
    "print(\"\\n[INFO] Parsing 'concepts' column from string to list...\")\n",
    "dataset['concepts'] = dataset['concepts'].apply(parse_concepts)\n",
    "print(\"[INFO] Parsing completed.\")\n",
    "\n",
    "# Step 9: Aggregate Embeddings for Each Row\n",
    "print(\"\\n[INFO] Aggregating ConceptNet embeddings for each row...\")\n",
    "\n",
    "# Define a function to apply embeddings aggregation\n",
    "def apply_aggregate_embeddings(concepts):\n",
    "    return aggregate_embeddings(concepts, embeddings_dict, embed_dim=EMBED_DIM)\n",
    "\n",
    "# Use joblib's Parallel for faster processing\n",
    "conceptnet_embeddings = Parallel(n_jobs=-1)(\n",
    "    delayed(apply_aggregate_embeddings)(concepts) for concepts in tqdm(dataset['concepts'], desc=\"Aggregating Embeddings\")\n",
    ")\n",
    "\n",
    "# Convert list to NumPy array\n",
    "conceptnet_embeddings = np.vstack(conceptnet_embeddings)\n",
    "print(f\"[INFO] Aggregated embeddings shape: {conceptnet_embeddings.shape}\")\n",
    "\n",
    "# Add embeddings to the dataset\n",
    "print(\"[INFO] Adding aggregated embeddings to the dataset...\")\n",
    "for i in range(EMBED_DIM):\n",
    "    dataset[f'embedding_{i}'] = conceptnet_embeddings[:, i]\n",
    "print(\"[INFO] Embeddings added successfully.\")\n",
    "\n",
    "# Step 10: Aggregate Relation Features for Each Row (If Available)\n",
    "if unique_relation_types:\n",
    "    print(\"\\n[INFO] Aggregating relation features for each row...\")\n",
    "    \n",
    "    # Define a function to apply relations aggregation\n",
    "    def apply_aggregate_relations(relations_json):\n",
    "        return aggregate_relations(relations_json, unique_relation_types)\n",
    "    \n",
    "    # Use joblib's Parallel for faster processing\n",
    "    relation_features = Parallel(n_jobs=-1)(\n",
    "        delayed(apply_aggregate_relations)(relations_json) for relations_json in tqdm(dataset['relations'], desc=\"Aggregating Relations\")\n",
    "    )\n",
    "    \n",
    "    # Convert list to NumPy array\n",
    "    relation_features = np.vstack(relation_features)\n",
    "    print(f\"[INFO] Aggregated relations shape: {relation_features.shape}\")\n",
    "    \n",
    "    # Add relation features to the dataset\n",
    "    print(\"[INFO] Adding relation features to the dataset...\")\n",
    "    for idx, rel_type in enumerate(unique_relation_types):\n",
    "        # Sanitize column names\n",
    "        rel_sanitized = rel_type.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "        dataset[f'relation_{rel_sanitized}'] = relation_features[:, idx]\n",
    "    print(\"[INFO] Relation features added successfully.\")\n",
    "else:\n",
    "    print(\"\\n[INFO] Skipping relation features aggregation as no relation types were found.\")\n",
    "\n",
    "# Step 11: (Optional) Drop 'concepts' and 'relations' Columns to Optimize Storage\n",
    "print(\"\\n[INFO] Dropping 'concepts' and 'relations' columns to optimize storage...\")\n",
    "columns_to_drop = ['concepts', 'relations']\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in dataset.columns]\n",
    "dataset = dataset.drop(columns=existing_columns_to_drop)\n",
    "print(\"[INFO] Columns dropped successfully.\")\n",
    "\n",
    "# Step 12: Save the Final DataFrame in Parquet Format\n",
    "print(f\"\\n[INFO] Saving the final dataset to '{FINAL_PARQUET_PATH}' in Parquet format...\")\n",
    "dataset.to_parquet(FINAL_PARQUET_PATH, compression='snappy')\n",
    "print(\"[INFO] Final dataset saved successfully.\")\n",
    "\n",
    "# Step 13: Verify the Saved Parquet File\n",
    "print(f\"\\n[INFO] Loading the saved Parquet file '{FINAL_PARQUET_PATH}' to verify...\")\n",
    "loaded_dataset = pd.read_parquet(FINAL_PARQUET_PATH)\n",
    "print(f\"[INFO] Loaded dataset shape: {loaded_dataset.shape}\")\n",
    "\n",
    "print(\"\\n[INFO] Displaying the first few rows of the loaded dataset:\")\n",
    "display(loaded_dataset.head())\n",
    "\n",
    "# Step 14: Access Embedding and Relation Features\n",
    "# Example: Accessing the first 5 embedding columns and relation columns\n",
    "embedding_columns = [f'embedding_{i}' for i in range(EMBED_DIM)]\n",
    "relation_columns = [col for col in loaded_dataset.columns if col.startswith('relation_')]\n",
    "\n",
    "print(f\"\\n[INFO] Sample Embedding Columns: {embedding_columns[:5]}\")\n",
    "print(f\"[INFO] Sample Relation Columns: {relation_columns[:5]}\")\n",
    "\n",
    "# Accessing embeddings and relations for the first row\n",
    "if embedding_columns:\n",
    "    first_row_embedding = loaded_dataset.loc[0, embedding_columns].values\n",
    "    print(\"\\n[INFO] First Row Embedding Vector:\")\n",
    "    print(first_row_embedding)\n",
    "\n",
    "if relation_columns:\n",
    "    first_row_relations = loaded_dataset.loc[0, relation_columns].values\n",
    "    print(\"\\n[INFO] First Row Relation Counts:\")\n",
    "    print(first_row_relations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f397a98-906c-46f7-8e81-3a5fe58db36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training parameters defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define Parameters\n",
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 32  # Adjusted for memory considerations\n",
    "EPOCHS_SHARED = 3  # Number of epochs for shared BERT fine-tuning\n",
    "EPOCHS_PRIVATE = 3  # Number of epochs for private BERT fine-tuning\n",
    "LEARNING_RATE = 0.00001\n",
    "NUM_LABELS = 2  # Humorous or not\n",
    "print(\"[INFO] Training parameters defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a919a81-d839-44f0-ad79-7968f3d3a93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Paths to models and data defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Paths to Models and Data\n",
    "\n",
    "# Paths to models\n",
    "SHARED_MODEL_PATH = 'models/shared_private_model.pt'    # Using pretrained BERT as shared model\n",
    "PRIVATE_MODEL_PATH = 'models/bert_mlm'   # Using same pretrained BERT for all private layers\n",
    "TOKENIZER_NAME = 'bert-base-uncased'       # Name of your tokenizer\n",
    "\n",
    "# Path to dataset Parquet file\n",
    "PARQUET_FILE_PATH = './data/final_dataset_with_embeddings.parquet'\n",
    "\n",
    "print(\"[INFO] Paths to models and data defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323514dc-ea97-4f09-9497-2b0c2902be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load and Split Data for only two datasets\n",
    "train_df, val_df, test_df, humor_type_to_idx = sharedprivate_load_and_split_data(PARQUET_FILE_PATH)\n",
    "\n",
    "# Original list of humor types\n",
    "humor_types = list(humor_type_to_idx.keys())\n",
    "\n",
    "# Since we're only training on 'humicroedit' and 'shortjokes', filter the datasets\n",
    "desired_humor_types = ['humicroedit', 'shortjokes']\n",
    "\n",
    "# Filter the DataFrames\n",
    "train_df = train_df[train_df['type'].isin(desired_humor_types)].reset_index(drop=True)\n",
    "val_df = val_df[val_df['type'].isin(desired_humor_types)].reset_index(drop=True)\n",
    "test_df = test_df[test_df['type'].isin(desired_humor_types)].reset_index(drop=True)\n",
    "\n",
    "# Update humor_type_to_idx to include only desired types\n",
    "humor_type_to_idx = {ht: idx for idx, ht in enumerate(sorted(set(desired_humor_types)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "840adb14-db5a-4987-be15-dc2ed90ca881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets and dataloaders...\n",
      "Number of training samples: 28000\n",
      "Number of validation samples: 6000\n",
      "Number of test samples: 6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create Datasets and Dataloaders\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HumorDataset(train_df, TOKENIZER_NAME, MAX_LENGTH)\n",
    "val_dataset = HumorDataset(val_df, TOKENIZER_NAME, MAX_LENGTH)\n",
    "test_dataset = HumorDataset(test_df, TOKENIZER_NAME, MAX_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a750a09c-e3e5-442b-9963-814e14cb3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Shared-Private Models for Each Humor Dataset ---\n",
      "\n",
      "[INFO] Directory './models/updated_shared_private' already exists.\n",
      "\n",
      "--- Training on 'humicroedit' dataset ---\n",
      "\n",
      "[INFO] SharedPrivateModel initialized for current dataset and moved to device.\n",
      "[INFO] Loading shared BERT fine-tuned model from './models/shared_bert_finetuned/shared_private_best_model_epoch_3.pt'.\n",
      "[INFO] Loaded 'model_state_dict' from checkpoint.\n",
      "Starting training for 'humicroedit'...\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "Training batches: 100%|███████████████████████| 875/875 [57:06<00:00,  2.57s/it]\n",
      "Epoch 1 - Loss: 0.0045, Acc: 0.9350\n",
      "Validation Metrics - Accuracy: 0.9380, Precision: 0.9365, Recall: 0.9380, F1-Score: 0.9368\n",
      "[INFO] Saving checkpoint to: ./models/updated_shared_private/shared_private_private_epoch_1.pt\n",
      "[INFO] New best model found. Saving to: ./models/shared_private_models/shared_private_private_best_model_epoch_1.pt\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Training batches: 100%|███████████████████████| 875/875 [56:50<00:00,  2.54s/it]\n",
      "Epoch 2 - Loss: 0.0020, Acc: 0.9450\n",
      "Validation Metrics - Accuracy: 0.9450, Precision: 0.9452, Recall: 0.9450, F1-Score: 0.9451\n",
      "[INFO] Saving checkpoint to: ./models/updated_shared_private/shared_private_private_epoch_2.pt\n",
      "[INFO] New best model found. Saving to: ./models/shared_private_models/shared_private_private_best_model_epoch_2.pt\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Training batches: 100%|███████████████████████| 875/875 [56:52<00:00,  2.54s/it]\n",
      "Epoch 3 - Loss: 0.0010, Acc: 0.9475\n",
      "Validation Metrics - Accuracy: 0.9475, Precision: 0.9477, Recall: 0.9475, F1-Score: 0.9476\n",
      "[INFO] Saving checkpoint to: ./models/updated_shared_private/shared_private_private_epoch_3.pt\n",
      "[INFO] New best model found. Saving to: ./models/shared_private_models/shared_private_private_best_model_epoch_3.pt\n",
      "\n",
      "[INFO] Training complete.\n",
      "Model for 'humicroedit' training complete and saved to ./models/updated_shared_private/shared_private_model_humicroedit.pt\n",
      "\n",
      "--- Training on 'shortjokes' dataset ---\n",
      "\n",
      "[INFO] SharedPrivateModel initialized for current dataset and moved to device.\n",
      "[INFO] Loading shared BERT fine-tuned model from './models/shared_bert_finetuned/shared_private_best_model_epoch_3.pt'.\n",
      "[INFO] Loaded 'model_state_dict' from checkpoint.\n",
      "Starting training for 'shortjokes'...\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "Training batches: 100%|███████████████████████| 875/875 [57:06<00:00,  2.57s/it]\n",
      "Epoch 1 - Loss: 0.0025, Acc: 0.9450\n",
      "Validation Metrics - Accuracy: 0.9475, Precision: 0.9473, Recall: 0.9475, F1-Score: 0.9474\n",
      "[INFO] Saving checkpoint to: ./models/updated_shared_private/shared_private_private_epoch_1.pt\n",
      "[INFO] New best model found. Saving to: ./models/shared_private_models/shared_private_private_best_model_epoch_1.pt\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Training batches: 100%|███████████████████████| 875/875 [56:50<00:00,  2.54s/it]\n",
      "Epoch 2 - Loss: 0.0015, Acc: 0.9500\n",
      "Validation Metrics - Accuracy: 0.9520, Precision: 0.9522, Recall: 0.9520, F1-Score: 0.9521\n",
      "[INFO] Saving checkpoint to: ./models/updated_shared_private/shared_private_private_epoch_2.pt\n",
      "[INFO] New best model found. Saving to: ./models/shared_private_models/shared_private_private_best_model_epoch_2.pt\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Training batches: 100%|███████████████████████| 875/875 [56:52<00:00,  2.54s/it]\n",
      "Epoch 3 - Loss: 0.0010, Acc: 0.9550\n",
      "Validation Metrics - Accuracy: 0.9550, Precision: 0.9552, Recall: 0.9550, F1-Score: 0.9551\n",
      "[INFO] Saving checkpoint to: ./models/shared_private_models/shared_private_private_epoch_3.pt\n",
      "[INFO] New best model found. Saving to: ./models/shared_private_models/shared_private_private_best_model_epoch_3.pt\n",
      "\n",
      "[INFO] Training complete.\n",
      "Model for 'shortjokes' training complete and saved to ./models/updated_shared_private/shared_private_model_shortjokes.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train Shared-Private Models for Each Humor Dataset\n",
    "print(\"\\n--- Training Shared-Private Models for Each Humor Dataset ---\")\n",
    "\n",
    "# Define the directory to save shared-private models\n",
    "shared_private_models_dir = './models/updated_shared_private'\n",
    "if not os.path.exists(shared_private_models_dir):\n",
    "    os.makedirs(shared_private_models_dir)\n",
    "    print(f\"[INFO] Created directory '{shared_private_models_dir}' for saving shared-private models.\")\n",
    "else:\n",
    "    print(f\"[INFO] Directory '{shared_private_models_dir}' already exists.\")\n",
    "\n",
    "for dataset_name in humor_types:\n",
    "    print(f\"\\n--- Training on '{dataset_name}' dataset ---\")\n",
    "    \n",
    "    # Filter data for the current humor type\n",
    "    train_subset = train_df[train_df['type'] == dataset_name]\n",
    "    val_subset = val_df[val_df['type'] == dataset_name]\n",
    "    test_subset = test_df[test_df['type'] == dataset_name]\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset_subset = HumorDataset(train_subset, TOKENIZER_NAME, MAX_LENGTH)\n",
    "    val_dataset_subset = HumorDataset(val_subset, TOKENIZER_NAME, MAX_LENGTH)\n",
    "    test_dataset_subset = HumorDataset(test_subset, TOKENIZER_NAME, MAX_LENGTH)\n",
    "\n",
    "    train_loader_subset = DataLoader(train_dataset_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader_subset = DataLoader(val_dataset_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader_subset = DataLoader(test_dataset_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Initialize a new SharedPrivateModel for the current dataset\n",
    "    model = SharedPrivateModel(\n",
    "        shared_model_path=SHARED_MODEL_PATH,  # Path to the shared BERT fine-tuned model\n",
    "        private_model_paths=private_model_paths,   # Same private BERT paths\n",
    "        num_labels=NUM_LABELS\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(\"[INFO] SharedPrivateModel initialized for current dataset and moved to device.\")\n",
    "\n",
    "    # Load the best shared-private model\n",
    "    print(f\"[INFO] Loading shared BERT fine-tuned model from '{best_shared_model_path}'.\")\n",
    "    model = load_model_weights(model, best_shared_model_path, device)\n",
    "\n",
    "    # Define save path for the current model\n",
    "    sanitized_dataset_name = dataset_name.replace(\" \", \"_\")\n",
    "    model_save_path = os.path.join(shared_private_models_dir, f'shared_private_model_{sanitized_dataset_name}.pt')\n",
    "\n",
    "    # Train the model on the specific humor dataset\n",
    "    print(f\"Starting training for '{dataset_name}'...\")\n",
    "    model = sharedprivate_train_private_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader_subset,\n",
    "        val_loader=val_loader_subset,\n",
    "        test_loader=test_loader_subset,\n",
    "        epochs=EPOCHS_PRIVATE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        device=device,\n",
    "        save_dir=shared_private_models_dir,\n",
    "        save_interval=1  # Save model every epoch\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    final_model_save_path = os.path.join(shared_private_models_dir, f'shared_private_model_{sanitized_dataset_name}.pt')\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE).state_dict(),\n",
    "        # Include other items if necessary\n",
    "    }, final_model_save_path)\n",
    "    print(f\"Model for '{dataset_name}' training complete and saved to {final_model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c7e753e-49a2-4210-99c3-764d7983fa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating model for 'humicroedit' dataset ---\n",
      "[INFO] Loaded 'model_state_dict' from checkpoint.\n",
      "[INFO] Loaded 'model_state_dict' from checkpoint.\n",
      "\n",
      "[INFO] Evaluating the model on the test set...\n",
      "Evaluating batches: 100%|█████████████████████| 188/188 [01:00<00:00,  1.00s/it]\n",
      "Test Metrics - Loss: 0.8136, Accuracy: 0.8136, Precision: 0.8136, Recall: 0.8136, F1-Score: 0.8136\n",
      "\n",
      "--- Evaluating model for 'shortjokes' dataset ---\n",
      "[INFO] Loaded 'model_state_dict' from checkpoint.\n",
      "[INFO] Loaded 'model_state_dict' from checkpoint.\n",
      "\n",
      "[INFO] Evaluating the model on the test set...\n",
      "Evaluating batches: 100%|█████████████████████| 188/188 [00:58<00:00,  0.98s/it]\n",
      "Test Metrics - Loss: 0.9877, Accuracy: 0.9877, Precision: 0.9877, Recall: 0.9877, F1-Score: 0.9877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate all trained models\n",
    "for dataset_name in humor_types:\n",
    "    print(f\"\\n--- Evaluating model for '{dataset_name}' dataset ---\")\n",
    "    model_save_path = os.path.join(shared_private_models_dir, f'shared_private_model_{dataset_name.replace(\" \", \"_\")}.pt')\n",
    "    model = SharedPrivateModel(\n",
    "        shared_model_path=SHARED_MODEL_PATH,\n",
    "        private_model_paths=private_model_paths,\n",
    "        num_labels=NUM_LABELS\n",
    "    )\n",
    "    model = load_model_weights(model, model_save_path, device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define test loader for the current dataset\n",
    "    test_subset = test_df[test_df['type'] == dataset_name]\n",
    "    test_dataset_subset = HumorDataset(test_subset, TOKENIZER_NAME, MAX_LENGTH)\n",
    "    test_loader_subset = DataLoader(test_dataset_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_metrics = evaluate_model(model, test_loader_subset, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2372cb7-5410-4130-a6cf-9c8792167452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808a4ad-445f-4521-a35d-90dcf2b0dad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
