{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbf4cd9-0810-4989-a5d0-2f091121de40",
   "metadata": {},
   "source": [
    "# SharedPrivateArchitecture dataset by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5713269-a49c-41fa-8ff4-b5c5c8733499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing from the src directory\n",
    "import sys\n",
    "sys.path.append('./src')  # Ensure src is in the Python path\n",
    "\n",
    "# Import necessary modules and functions\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from data import HumorDataset, sharedprivate_load_and_split_data\n",
    "from models import SharedPrivateModel\n",
    "from training import sharedprivate_train_shared_bert, sharedprivate_train_private_model\n",
    "from utils import sharedprivate_eval_model\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca25677-a045-451b-b999-23a9399d2be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set Up Device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e770d7a-4fa5-442d-a416-b7fdddd16484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Parameters\n",
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 32  # Adjusted for memory considerations\n",
    "EPOCHS_SHARED = 3  # Number of epochs for shared BERT fine-tuning\n",
    "EPOCHS_PRIVATE = 3  # Number of epochs for private BERT fine-tuning\n",
    "LEARNING_RATE = 0.00001\n",
    "NUM_LABELS = 2  # Humorous or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5104d1eb-a404-483a-9429-7e7a557732f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Paths to Models and Data\n",
    "\n",
    "# Paths to models\n",
    "SHARED_MODEL_PATH = './models/bert-classification'    # Path to your saved shared BERT model\n",
    "PRIVATE_MODEL_PATH = './models/bert-mlm'             # Path to your pre-trained BERT-MLM model\n",
    "TOKENIZER_NAME = 'bert-base-uncased'                 # Name of your tokenizer\n",
    "\n",
    "# Path to dataset CSV file\n",
    "CSV_FILE_PATH = './data/shared_private_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3431c70-cf4e-4c1e-96f7-d00bd9008de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and splitting data...\n",
      "Original Humor types: ['body punchlines', 'news headlines', 'puns', 'storylines']\n",
      "Original Humor type to index mapping: {'body punchlines': 0, 'news headlines': 1, 'puns': 2, 'storylines': 3}\n",
      "Counting filtered humor types in datasets...\n",
      "\n",
      "Training Humor Type Counts:\n",
      "body punchlines    14984\n",
      "storylines         14824\n",
      "puns               14798\n",
      "news headlines     14791\n",
      "Name: type, dtype: int64\n",
      "\n",
      "Validation Humor Type Counts:\n",
      "storylines         3222\n",
      "puns               3193\n",
      "body punchlines    3189\n",
      "news headlines     3124\n",
      "Name: type, dtype: int64\n",
      "\n",
      "Test Humor Type Counts:\n",
      "puns               3256\n",
      "storylines         3198\n",
      "body punchlines    3152\n",
      "news headlines     3122\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load and Split Data\n",
    "print(\"Loading and splitting data...\")\n",
    "train_df, val_df, test_df, humor_type_to_idx = sharedprivate_load_and_split_data(CSV_FILE_PATH)\n",
    "\n",
    "# Original list of humor types\n",
    "humor_types = list(humor_type_to_idx.keys())\n",
    "print(f\"Original Humor types: {humor_types}\")\n",
    "print(f\"Original Humor type to index mapping: {humor_type_to_idx}\")\n",
    "\n",
    "\n",
    "# Count humor types in the filtered train, validation, and test datasets\n",
    "print(\"Counting filtered humor types in datasets...\")\n",
    "\n",
    "def count_humor_types(df, dataset_name):\n",
    "    if 'type' in df.columns:  # Use 'type' as the column name for humor type\n",
    "        counts = df['type'].value_counts()\n",
    "        print(f\"\\n{dataset_name} Humor Type Counts:\")\n",
    "        print(counts)\n",
    "    else:\n",
    "        print(f\"Column 'type' not found in {dataset_name} dataset.\")\n",
    "\n",
    "count_humor_types(train_df, \"Training\")\n",
    "count_humor_types(val_df, \"Validation\")\n",
    "count_humor_types(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8c84dc-692a-4ea3-ae56-313168d13df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets and dataloaders...\n",
      "Number of training samples: 59397\n",
      "Number of validation samples: 12728\n",
      "Number of test samples: 12728\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create Datasets and Dataloaders\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HumorDataset(train_df, TOKENIZER_NAME, MAX_LENGTH)\n",
    "val_dataset = HumorDataset(val_df, TOKENIZER_NAME, MAX_LENGTH)\n",
    "test_dataset = HumorDataset(test_df, TOKENIZER_NAME, MAX_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a3b1fa-1a6d-4ba8-a677-6e020c89aab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./models/bert-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./models/bert-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./models/bert-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing and training the shared BERT model...\n",
      "[INFO] No checkpoint found in './models/bert-classification'. Loading shared BERT using 'from_pretrained'.\n",
      "[INFO] Shared BERT loaded from directory successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./models/bert-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Shared BERT weights loaded successfully.\n",
      "[INFO] Private BERT weights loaded successfully.\n",
      "[INFO] Classifier weights loaded successfully.\n",
      "Model found at ./models/shared_bert_finetuned/shared_private_best_model.pt. Skipping training and loading the model.\n",
      "Error: model_state_dict not found in the checkpoint!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The saved checkpoint does not contain a valid model_state_dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m shared_model \u001b[38;5;241m=\u001b[39m shared_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Train the shared-private model (which effectively fine-tunes the private BERT and classifier)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m shared_model \u001b[38;5;241m=\u001b[39m \u001b[43msharedprivate_train_shared_bert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_SHARED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_model_save_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save model every epoch\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned shared-private model\u001b[39;00m\n\u001b[1;32m     72\u001b[0m final_shared_model_save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(shared_model_save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshared_private_best_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Masters/EE8228/Phase 2/Humor2/./src/training.py:487\u001b[0m, in \u001b[0;36msharedprivate_train_shared_bert\u001b[0;34m(model, train_loader, val_loader, epochs, learning_rate, device, save_dir, save_interval)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: model_state_dict not found in the checkpoint!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe saved checkpoint does not contain a valid model_state_dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo existing model found. Starting training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mValueError\u001b[0m: The saved checkpoint does not contain a valid model_state_dict."
     ]
    }
   ],
   "source": [
    "# Cell 7: Initialize and Train Shared BERT Model\n",
    "print(\"Initializing and training the shared BERT model...\")\n",
    "\n",
    "# Define the directory to save the shared BERT fine-tuned model\n",
    "shared_model_save_dir = './models/shared_bert_finetuned'\n",
    "if not os.path.exists(shared_model_save_dir):\n",
    "    os.makedirs(shared_model_save_dir)\n",
    "\n",
    "# Define private model paths for each humor type\n",
    "# Assuming you have separate pre-trained MLM BERT models for each humor type.\n",
    "# Alternatively, you can use the same MLM-pretrained BERT for all private layers initially.\n",
    "# Here, we'll assume the same MLM BERT is used for all, but you can adjust as needed.\n",
    "\n",
    "private_model_paths = {humor_type_to_idx[ht]: PRIVATE_MODEL_PATH for ht in humor_types}\n",
    "\n",
    "# Initialize the SharedPrivateModel\n",
    "shared_model = SharedPrivateModel(\n",
    "    shared_model_path=SHARED_MODEL_PATH,\n",
    "    private_model_paths=private_model_paths,  # Dictionary mapping humor type indices to private BERT paths\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "def load_model_weights(model, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    # Load shared BERT weights\n",
    "    shared_bert_state_dict = {\n",
    "        k.replace(\"shared_bert.\", \"\"): v\n",
    "        for k, v in checkpoint.items()\n",
    "        if k.startswith(\"shared_bert.\")\n",
    "    }\n",
    "    model.shared_bert.load_state_dict(shared_bert_state_dict, strict=False)\n",
    "    print(\"[INFO] Shared BERT weights loaded successfully.\")\n",
    "\n",
    "    # Load private BERT weights\n",
    "    for humor_type_idx, private_model in model.private_bert_dict.items():\n",
    "        private_bert_state_dict = {\n",
    "            k.replace(f\"private_bert_dict.{humor_type_idx}.\", \"\"): v\n",
    "            for k, v in checkpoint.items()\n",
    "            if k.startswith(f\"private_bert_dict.{humor_type_idx}.\")\n",
    "        }\n",
    "        private_model.load_state_dict(private_bert_state_dict, strict=False)\n",
    "    print(\"[INFO] Private BERT weights loaded successfully.\")\n",
    "\n",
    "    # Load classifier weights\n",
    "    classifier_state_dict = {\n",
    "        k.replace(\"classifier.\", \"\"): v\n",
    "        for k, v in checkpoint.items()\n",
    "        if k.startswith(\"classifier.\")\n",
    "    }\n",
    "    model.classifier.load_state_dict(classifier_state_dict, strict=False)\n",
    "    print(\"[INFO] Classifier weights loaded successfully.\")\n",
    "\n",
    "    return model\n",
    "    \n",
    "shared_mdel = load_model_weights(shared_model , './models/shared_bert_finetuned/shared_private_best_model.pt')\n",
    "\n",
    "shared_model = shared_model.to(device)\n",
    "\n",
    "# Train the shared-private model (which effectively fine-tunes the private BERT and classifier)\n",
    "shared_model = sharedprivate_train_shared_bert(\n",
    "    model=shared_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS_SHARED,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device,\n",
    "    save_dir=shared_model_save_dir,\n",
    "    save_interval=1  # Save model every epoch\n",
    ")\n",
    "\n",
    "# Save the fine-tuned shared-private model\n",
    "final_shared_model_save_path = os.path.join(shared_model_save_dir, 'shared_private_best_model.pt')\n",
    "torch.save(shared_model.state_dict(), final_shared_model_save_path)\n",
    "print(f\"Shared BERT fine-tuned model saved to {final_shared_model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8db3d4-e2b8-40e5-b247-10a93aae0e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches:   8%|█▊                    | 33/398 [00:33<04:42,  1.29it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x116030c10>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/manav/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/manav/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/manav/miniconda3/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/manav/miniconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Users/manav/miniconda3/lib/python3.9/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/manav/miniconda3/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Evaluating batches:   8%|█▊                    | 33/398 [00:39<07:20,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m \u001b[43msharedprivate_eval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Metrics - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Masters/EE8228/Phase 2/Humor2/./src/utils.py:37\u001b[0m, in \u001b[0;36msharedprivate_eval_model\u001b[0;34m(model, data_loader, loss_fn, device, n_examples)\u001b[0m\n\u001b[1;32m     34\u001b[0m humor_type_idx \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumor_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhumor_type_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels)\n\u001b[1;32m     40\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masters/EE8228/Phase 2/Humor2/./src/models.py:85\u001b[0m, in \u001b[0;36mSharedPrivateModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, humor_type_idx)\u001b[0m\n\u001b[1;32m     83\u001b[0m private_pooled_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[0;32m---> 85\u001b[0m     humor_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mhumor_type_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     86\u001b[0m     private_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivate_bert_dict[humor_type]\n\u001b[1;32m     87\u001b[0m     private_output \u001b[38;5;241m=\u001b[39m private_model(\n\u001b[1;32m     88\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     89\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     90\u001b[0m         token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     91\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_metrics = sharedprivate_eval_model(shared_model, test_loader, criterion, device, len(test_loader.dataset))\n",
    "print(\n",
    "    f\"Test Metrics - Accuracy: {test_metrics['accuracy']:.4f}, \"\n",
    "    f\"Precision: {test_metrics['precision']:.4f}, \"\n",
    "    f\"Recall: {test_metrics['recall']:.4f}, \"\n",
    "    f\"F1-Score: {test_metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85667564-b6bd-4d37-b2d2-f6ca4c428e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Shared-Private Models for Each Humor Dataset ---\n",
      "\n",
      "--- Training on puns dataset ---\n",
      "[INFO] Found checkpoint 'shared_private_best_model.pt' in './models/shared_bert_finetuned'. Loading shared BERT from checkpoint.\n",
      "[INFO] Shared BERT loaded from checkpoint successfully.\n",
      "Loaded shared BERT fine-tuned model from ./models/shared_bert_finetuned/shared_private_best_model.pt\n",
      "Starting training for body punchlines...\n",
      "Training shared-private model on device: mps\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "Training batches: 100%|███████████████████████| 469/469 [20:06<00:00,  2.57s/it]\n",
      "Epoch 1 - Loss: 0.0085, Acc: 0.9221\n",
      "Evaluating batches: 100%|█████████████████████| 100/100 [01:45<00:00,  1.06s/it]\n",
      "Validation Metrics - Accuracy: 0.9260, Precision: 0.9245, Recall: 0.9260, F1-Score: 0.9252\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_1.pt\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Training batches: 100%|███████████████████████| 469/469 [19:50<00:00,  2.54s/it]\n",
      "Epoch 2 - Loss: 0.0032, Acc: 0.9310\n",
      "Evaluating batches: 100%|█████████████████████| 100/100 [01:44<00:00,  1.04s/it]\n",
      "Validation Metrics - Accuracy: 0.9315, Precision: 0.9318, Recall: 0.9310, F1-Score: 0.9314\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_2.pt\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Training batches: 100%|███████████████████████| 469/469 [19:52<00:00,  2.54s/it]\n",
      "Epoch 3 - Loss: 0.0010, Acc: 0.9325\n",
      "Evaluating batches: 100%|█████████████████████| 100/100 [01:44<00:00,  1.04s/it]\n",
      "Validation Metrics - Accuracy: 0.9325, Precision: 0.9327, Recall: 0.9325, F1-Score: 0.9326\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_3.pt\n",
      "New best model found. Saving to: ./models/updated_shared_private/shared_private_best_model_3.pt\n",
      "\n",
      "Evaluating on test set...\n",
      "Evaluating batches: 100%|█████████████████████| 99/99 [01:44<00:00,  1.06s/it]\n",
      "Test Metrics - Accuracy: 0.9325, Precision: 0.9325, Recall: 0.9325, F1-Score: 0.9325\n",
      "Model for body punchlines training complete and saved to ./models/updated_shared_private/shared_private_model_body_punchlines.pt\n",
      "\n",
      "--- Training on Reddit dataset ---\n",
      "[INFO] Found checkpoint 'shared_private_best_model.pt' in './models/shared_bert_finetuned'. Loading shared BERT from checkpoint.\n",
      "[INFO] Shared BERT loaded from checkpoint successfully.\n",
      "Loaded shared BERT fine-tuned model from ./models/shared_bert_finetuned/shared_private_best_model.pt\n",
      "Starting training for Reddit...\n",
      "Training shared-private model on device: mps\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "Training batches: 100%|███████████████████████| 463/463 [19:47<00:00,  2.57s/it]\n",
      "Epoch 1 - Loss: 0.3123, Acc: 0.7154\n",
      "Evaluating batches: 100%|███████████████████████| 98/98 [01:43<00:00,  1.06s/it]\n",
      "Validation Metrics - Accuracy: 0.7202, Precision: 0.7220, Recall: 0.7202, F1-Score: 0.7211\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_1.pt\n",
      "New best model found. Saving to: ./models/updated_shared_private/shared_private_best_model_1.pt\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Training batches: 100%|███████████████████████| 463/463 [19:36<00:00,  2.54s/it]\n",
      "Epoch 2 - Loss: 0.2781, Acc: 0.7308\n",
      "Evaluating batches: 100%|███████████████████████| 98/98 [01:43<00:00,  1.06s/it]\n",
      "Validation Metrics - Accuracy: 0.7330, Precision: 0.7345, Recall: 0.7330, F1-Score: 0.7337\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_2.pt\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Training batches: 100%|███████████████████████| 463/463 [19:48<00:00,  2.57s/it]\n",
      "Epoch 3 - Loss: 0.2458, Acc: 0.7355\n",
      "Evaluating batches: 100%|███████████████████████| 98/98 [01:43<00:00,  1.06s/it]\n",
      "Validation Metrics - Accuracy: 0.7355, Precision: 0.7356, Recall: 0.7355, F1-Score: 0.7355\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_3.pt\n",
      "\n",
      "Evaluating on test set...\n",
      "Evaluating batches: 100%|███████████████████████| 98/98 [01:43<00:00,  1.06s/it]\n",
      "Test Metrics - Accuracy: 0.7355, Precision: 0.7355, Recall: 0.7355, F1-Score: 0.7355\n",
      "Model for Reddit training complete and saved to ./models/updated_shared_private/shared_private_model_reddit.pt\n",
      "\n",
      "--- Training on Humicroedit dataset ---\n",
      "[INFO] Found checkpoint 'shared_private_best_model.pt' in './models/shared_bert_finetuned'. Loading shared BERT from checkpoint.\n",
      "[INFO] Shared BERT loaded from checkpoint successfully.\n",
      "Loaded shared BERT fine-tuned model from ./models/shared_bert_finetuned/shared_private_best_model.pt\n",
      "Starting training for Humicroedit...\n",
      "Training shared-private model on device: mps\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "Training batches: 100%|███████████████████████| 463/463 [19:45<00:00,  2.56s/it]\n",
      "Epoch 1 - Loss: 0.0641, Acc: 0.7953\n",
      "Evaluating batches: 100%|█████████████████████| 100/100 [01:46<00:00,  1.06s/it]\n",
      "Validation Metrics - Accuracy: 0.8010, Precision: 0.8007, Recall: 0.8010, F1-Score: 0.8008\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_1.pt\n",
      "New best model found. Saving to: ./models/updated_shared_private/shared_private_best_model_1.pt\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Training batches: 100%|███████████████████████| 463/463 [19:36<00:00,  2.54s/it]\n",
      "Epoch 2 - Loss: 0.0403, Acc: 0.8103\n",
      "Evaluating batches: 100%|█████████████████████| 100/100 [01:45<00:00,  1.06s/it]\n",
      "Validation Metrics - Accuracy: 0.8136, Precision: 0.8138, Recall: 0.8136, F1-Score: 0.8137\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_2.pt\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Training batches: 100%|███████████████████████| 463/463 [19:43<00:00,  2.56s/it]\n",
      "Epoch 3 - Loss: 0.0218, Acc: 0.8136\n",
      "Evaluating batches: 100%|█████████████████████| 100/100 [01:45<00:00,  1.05s/it]\n",
      "Validation Metrics - Accuracy: 0.8136, Precision: 0.8136, Recall: 0.8136, F1-Score: 0.8136\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_3.pt\n",
      "\n",
      "Evaluating on test set...\n",
      "Evaluating batches: 100%|█████████████████████| 100/100 [01:45<00:00,  1.05s/it]\n",
      "Test Metrics - Accuracy: 0.8136, Precision: 0.8136, Recall: 0.8136, F1-Score: 0.8136\n",
      "Model for Humicroedit training complete and saved to ./models/updated_shared_private/shared_private_model_humicroedit.pt\n",
      "\n",
      "--- Training on Shortjokes dataset ---\n",
      "[INFO] Found checkpoint 'shared_private_best_model.pt' in './models/shared_bert_finetuned'. Loading shared BERT from checkpoint.\n",
      "[INFO] Shared BERT loaded from checkpoint successfully.\n",
      "Loaded shared BERT fine-tuned model from ./models/shared_bert_finetuned/shared_private_best_model.pt\n",
      "Starting training for Shortjokes...\n",
      "Training shared-private model on device: mps\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "Training batches: 100%|███████████████████████| 464/464 [19:41<00:00,  2.55s/it]\n",
      "Epoch 1 - Loss: 0.0045, Acc: 0.9852\n",
      "Evaluating batches: 100%|█████████████████████| 101/101 [01:44<00:00,  1.05s/it]\n",
      "Validation Metrics - Accuracy: 0.9864, Precision: 0.9862, Recall: 0.9864, F1-Score: 0.9863\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_1.pt\n",
      "New best model found. Saving to: ./models/updated_shared_private/shared_private_best_model_1.pt\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Training batches: 100%|███████████████████████| 464/464 [19:38<00:00,  2.54s/it]\n",
      "Epoch 2 - Loss: 0.0021, Acc: 0.9875\n",
      "Evaluating batches: 100%|█████████████████████| 101/101 [01:44<00:00,  1.05s/it]\n",
      "Validation Metrics - Accuracy: 0.9877, Precision: 0.9877, Recall: 0.9877, F1-Score: 0.9877\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_2.pt\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Training batches: 100%|███████████████████████| 464/464 [19:39<00:00,  2.54s/it]\n",
      "Epoch 3 - Loss: 0.0013, Acc: 0.9885\n",
      "Evaluating batches: 100%|█████████████████████| 101/101 [01:45<00:00,  1.05s/it]\n",
      "Validation Metrics - Accuracy: 0.9877, Precision: 0.9878, Recall: 0.9877, F1-Score: 0.9878\n",
      "Saving checkpoint to: ./models/updated_shared_private/shared_private_3.pt\n",
      "\n",
      "Evaluating on test set...\n",
      "Evaluating batches: 100%|█████████████████████| 100/100 [01:44<00:00,  1.05s/it]\n",
      "Test Metrics - Accuracy: 0.9877, Precision: 0.9877, Recall: 0.9877, F1-Score: 0.9877\n",
      "Model for Shortjokes training complete and saved to ./models/updated_shared_private/shared_private_model_shortjokes.pt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train Shared-Private Models for Each Humor Dataset\n",
    "print(\"\\n--- Training Shared-Private Models for Each Humor Dataset ---\")\n",
    "\n",
    "# Define the directory to save shared-private models\n",
    "shared_private_models_dir = './models/updated_shared_private'\n",
    "if not os.path.exists(shared_private_models_dir):\n",
    "    os.makedirs(shared_private_models_dir)\n",
    "\n",
    "for dataset_name in humor_types:\n",
    "    print(f\"\\n--- Training on {dataset_name} dataset ---\")\n",
    "    # Filter data for the current humor type\n",
    "    train_subset = train_df[train_df['type'] == dataset_name]\n",
    "    val_subset = val_df[val_df['type'] == dataset_name]\n",
    "    test_subset = test_df[test_df['type'] == dataset_name]\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = HumorDataset(train_subset, TOKENIZER_NAME, MAX_LENGTH)\n",
    "    val_dataset = HumorDataset(val_subset, TOKENIZER_NAME, MAX_LENGTH)\n",
    "    test_dataset = HumorDataset(test_subset, TOKENIZER_NAME, MAX_LENGTH)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Initialize a new SharedPrivateModel for the current dataset\n",
    "    # Reuse the shared BERT and use the same private BERT as trained above\n",
    "    # Alternatively, you can initialize separate private BERTs if desired\n",
    "\n",
    "    # Load the best shared-private model from shared_model_save_dir\n",
    "    model = SharedPrivateModel(\n",
    "        shared_model_path=shared_model_save_dir,  # Path to the shared BERT fine-tuned model\n",
    "        private_model_paths=private_model_paths,   # Same private BERT paths\n",
    "        num_labels=NUM_LABELS\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Load the best model state\n",
    "    best_shared_model_path = os.path.join(shared_model_save_dir, 'shared_private_best_model.pt')\n",
    "    model.load_state_dict(torch.load(best_shared_model_path))\n",
    "    print(f\"Loaded shared BERT fine-tuned model from {best_shared_model_path}\")\n",
    "\n",
    "    # Define save path for the current model\n",
    "    sanitized_dataset_name = dataset_name.replace(\" \", \"_\")\n",
    "    model_save_path = os.path.join(shared_private_models_dir, f'shared_private_model_{sanitized_dataset_name}.pt')\n",
    "\n",
    "    # Train the model on the specific humor dataset\n",
    "    print(f\"Starting training for {dataset_name}...\")\n",
    "    model = sharedprivate_train_private_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=EPOCHS_PRIVATE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        device=device,\n",
    "        save_dir=shared_private_models_dir,\n",
    "        save_interval=1  # Save model every epoch\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    final_model_save_path = os.path.join(shared_private_models_dir, f'shared_private_model_{sanitized_dataset_name}.pt')\n",
    "    torch.save(model.state_dict(), final_model_save_path)\n",
    "    print(f\"Model for {dataset_name} training complete and saved to {final_model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172846cc-117c-4049-820c-4c279013f01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b263b-207d-455e-8125-f32e967ebf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = './models/shared_bert_finetuned/shared_private_best_model.pt'\n",
    "\n",
    "# Load and print the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "print(\"Checkpoint keys:\", checkpoint.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe85be-845a-4c1d-9d98-24c969e6e3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
